{
  "name": "apache-flume-hdfs Juju charm",
  "juju_charm_name": "apache-flume-hdfs",
  "revision": "cs:trusty/apache-flume-hdfs-4",
  "latest": true,
  "uris": [
    "https://jujucharms.com/apache-flume-hdfs",
    "https://jujucharms.com/apache-flume-hdfs/trusty",
    "https://jujucharms.com/apache-flume-hdfs/trusty/4",
    "https://api.jujucharms.com/v5/apache-flume-hdfs",
    "https://api.jujucharms.com/v5/trusty/apache-flume-hdfs",
    "https://api.jujucharms.com/v5/trusty/apache-flume-hdfs-4"
  ],
  "labels": [
    "Juju charm",
    "applications",
    "bigdata",
    "hadoop",
    "apache",
    "Mode/Executable/Bundle/Juju Charm",
    "Type/Devopsware/Deployment/Juju",
    "Type/Middleware/Web Server/Apache HTTP Server"
  ],
  "info_url": "https://jujucharms.com/apache-flume-hdfs",
  "package_url": "https://api.jujucharms.com/v5/trusty/apache-flume-hdfs-4/archive",
  "created": "2015-10-07T21:01:53.481Z",
  "updated": "2015-10-07T21:01:53.481Z",
  "description": "Ingest data into HDFS\n\nCollect, aggregate, and move large amounts of data into HDFS.\n",
  "maintainer": {
    "name": "bigdata-charmers"
  },
  "juju_charm_subordinate": false,
  "juju_charm_series": "trusty",
  "juju_charm_owner": "bigdata-charmers",
  "requires": [
    {
      "kind": "host",
      "label": "Infrastructure/Operating System/Linux/Ubuntu",
      "version": "= trusty"
    }
  ],
  "parameters": {
    "channel_capacity": {
      "type": "int",
      "description": "The maximum number of events stored in the channel.\n",
      "default": 1000,
      "mapping": "charm_option"
    },
    "channel_transaction_capacity": {
      "type": "int",
      "description": "The maximum number of events the channel will take from a source or\ngive to a sink per transaction.\n",
      "default": 100,
      "mapping": "charm_option"
    },
    "dfs_replication": {
      "type": "int",
      "description": "The DFS replication value. The default (3) is the same as the Namenode\nprovided by apache-hadoop-hdfs-master, but may be overriden for this\nservice.\n",
      "default": 3,
      "mapping": "charm_option"
    },
    "protocol": {
      "type": "string",
      "description": "Ingestion protocol for the agent source. Currently only 'avro' is supported.\n",
      "default": "avro",
      "mapping": "charm_option"
    },
    "resources_mirror": {
      "type": "string",
      "description": "URL from which to fetch resources (e.g., Hadoop binaries) instead\nof Launchpad.\n",
      "default": "",
      "mapping": "charm_option"
    },
    "roll_count": {
      "type": "int",
      "description": "Number of events written to file before it is rolled. A value of 0 (the\ndefault) means never roll based on number of events.\n",
      "default": 0,
      "mapping": "charm_option"
    },
    "roll_interval": {
      "type": "int",
      "description": "Number of seconds to wait before rolling the current file. Default\nwill roll the file after 5 minutes. A value of 0 means never roll\nbased on a time interval.\n",
      "default": 300,
      "mapping": "charm_option"
    },
    "roll_size": {
      "type": "string",
      "description": "File size to trigger roll, in bytes. Default will roll the file once\nit reaches 10 MB. A value of 0 means never roll based on file size.\n",
      "default": "10000000",
      "mapping": "charm_option"
    },
    "sink_compression": {
      "type": "string",
      "description": "Compression codec for the agent sink. An empty value will write events\nto HDFS uncompressed. You may specify 'snappy' here to compress written\nevents using the snappy codec.\n",
      "default": "",
      "mapping": "charm_option"
    },
    "sink_serializer": {
      "type": "string",
      "description": "Specify the serializer used when the sink writes to HDFS. Either\n'avro_event' or 'text' are supported.\n",
      "default": "text",
      "mapping": "charm_option"
    },
    "source_port": {
      "type": "int",
      "description": "Port on which the agent source is listening.\n",
      "default": 4141,
      "mapping": "charm_option"
    }
  },
  "provides": [
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/flume-agent",
      "juju_interface": "flume-agent",
      "juju_name": "flume-agent",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/hadoop-plugin",
      "juju_interface": "hadoop-plugin",
      "juju_name": "hadoop-plugin",
      "juju_role": "provider",
      "juju_limit": 0
    }
  ],
  "license": "## Overview\n\nFlume is a distributed, reliable, and available service for efficiently\ncollecting, aggregating, and moving large amounts of log data. It has a simple\nand flexible architecture based on streaming data flows. It is robust and fault\ntolerant with tunable reliability mechanisms and many failover and recovery\nmechanisms. It uses a simple extensible data model that allows for online\nanalytic application. Learn more at [flume.apache.org](http://flume.apache.org).\n\nThis charm provides a Flume agent designed to ingest events into the shared\nfilesystem (HDFS) of a connected Hadoop cluster. It is meant to relate to\nother Flume agents such as `apache-flume-syslog` and `apache-flume-twitter`.\n\n\n## Usage\n\nThis charm leverages our pluggable Hadoop model with the `hadoop-plugin`\ninterface. This means that you will need to deploy a base Apache Hadoop cluster\nto run Flume. The suggested deployment method is to use the\n[apache-ingestion-flume](https://jujucharms.com/u/bigdata-dev/apache-ingestion-flume/)\nbundle. This will deploy the Apache Hadoop platform with a single Apache Flume\nunit that communicates with the cluster by relating to the\n`apache-hadoop-plugin` subordinate charm:\n\n    juju quickstart u/bigdata-dev/apache-ingestion-flume\n\nAlternatively, you may manually deploy the recommended environment as follows:\n\n    juju deploy apache-hadoop-hdfs-master hdfs-master\n    juju deploy apache-hadoop-yarn-master yarn-master\n    juju deploy apache-hadoop-compute-slave compute-slave\n    juju deploy apache-hadoop-plugin plugin\n    juju deploy apache-flume-hdfs flume-hdfs\n\n    juju add-relation yarn-master hdfs-master\n    juju add-relation compute-slave yarn-master\n    juju add-relation compute-slave hdfs-master\n    juju add-relation plugin yarn-master\n    juju add-relation plugin hdfs-master\n    juju add-relation flume-hdfs plugin\n\nThe deployment at this stage isn't very exciting, as the `flume-hdfs` service\nis waiting for other Flume agents to connect and send data. You'll probably\nwant to check out\n[apache-flume-syslog](https://jujucharms.com/apache-flume-syslog)\nor\n[apache-flume-twitter](https://jujucharms.com/apache-flume-twitter)\nto provide additional functionality for this deployment.\n\nWhen `flume-hdfs` receives data, it is stored in a `/user/flume/<event_dir>`\nHDFS subdirectory (configured by the connected Flume charm). You can quickly\nverify the data written to HDFS using the command line. SSH to the `flume-hdfs`\nunit, locate an event, and cat it:\n\n    juju ssh flume-hdfs/0\n    hdfs dfs -ls /user/flume/<event_dir>               # <-- find a date\n    hdfs dfs -ls /user/flume/<event_dir>/<yyyy-mm-dd>  # <-- find an event\n    hdfs dfs -cat /user/flume/<event_dir>/<yyyy-mm-dd>/FlumeData.<id>\n\nThis process works well for data serialized in `text` format (the default).\nFor data serialized in `avro` format, you'll need to copy the file locally\nand use the `dfs -text` command. For example, replace the `dfs -cat` command\nfrom above with the following to view files stored in `avro` format:\n\n    hdfs dfs -copyToLocal /user/flume/<event_dir>/<yyyy-mm-dd>/FlumeData.<id> /home/ubuntu/myFile.txt\n    hdfs dfs -text file:///home/ubuntu/myFile.txt\n\n\n## Contact Information\n\n- <bigdata@lists.ubuntu.com>\n\n\n## Help\n\n- [Apache Flume home page](http://flume.apache.org/)\n- [Apache Flume bug tracker](https://issues.apache.org/jira/browse/flume)\n- [Apache Flume mailing lists](https://flume.apache.org/mailinglists.html)\n- `#juju` on `irc.freenode.net`\n",
  "readme": "## Overview\n\nFlume is a distributed, reliable, and available service for efficiently\ncollecting, aggregating, and moving large amounts of log data. It has a simple\nand flexible architecture based on streaming data flows. It is robust and fault\ntolerant with tunable reliability mechanisms and many failover and recovery\nmechanisms. It uses a simple extensible data model that allows for online\nanalytic application. Learn more at [flume.apache.org](http://flume.apache.org).\n\nThis charm provides a Flume agent designed to ingest events into the shared\nfilesystem (HDFS) of a connected Hadoop cluster. It is meant to relate to\nother Flume agents such as `apache-flume-syslog` and `apache-flume-twitter`.\n\n\n## Usage\n\nThis charm leverages our pluggable Hadoop model with the `hadoop-plugin`\ninterface. This means that you will need to deploy a base Apache Hadoop cluster\nto run Flume. The suggested deployment method is to use the\n[apache-ingestion-flume](https://jujucharms.com/u/bigdata-dev/apache-ingestion-flume/)\nbundle. This will deploy the Apache Hadoop platform with a single Apache Flume\nunit that communicates with the cluster by relating to the\n`apache-hadoop-plugin` subordinate charm:\n\n    juju quickstart u/bigdata-dev/apache-ingestion-flume\n\nAlternatively, you may manually deploy the recommended environment as follows:\n\n    juju deploy apache-hadoop-hdfs-master hdfs-master\n    juju deploy apache-hadoop-yarn-master yarn-master\n    juju deploy apache-hadoop-compute-slave compute-slave\n    juju deploy apache-hadoop-plugin plugin\n    juju deploy apache-flume-hdfs flume-hdfs\n\n    juju add-relation yarn-master hdfs-master\n    juju add-relation compute-slave yarn-master\n    juju add-relation compute-slave hdfs-master\n    juju add-relation plugin yarn-master\n    juju add-relation plugin hdfs-master\n    juju add-relation flume-hdfs plugin\n\nThe deployment at this stage isn't very exciting, as the `flume-hdfs` service\nis waiting for other Flume agents to connect and send data. You'll probably\nwant to check out\n[apache-flume-syslog](https://jujucharms.com/apache-flume-syslog)\nor\n[apache-flume-twitter](https://jujucharms.com/apache-flume-twitter)\nto provide additional functionality for this deployment.\n\nWhen `flume-hdfs` receives data, it is stored in a `/user/flume/<event_dir>`\nHDFS subdirectory (configured by the connected Flume charm). You can quickly\nverify the data written to HDFS using the command line. SSH to the `flume-hdfs`\nunit, locate an event, and cat it:\n\n    juju ssh flume-hdfs/0\n    hdfs dfs -ls /user/flume/<event_dir>               # <-- find a date\n    hdfs dfs -ls /user/flume/<event_dir>/<yyyy-mm-dd>  # <-- find an event\n    hdfs dfs -cat /user/flume/<event_dir>/<yyyy-mm-dd>/FlumeData.<id>\n\nThis process works well for data serialized in `text` format (the default).\nFor data serialized in `avro` format, you'll need to copy the file locally\nand use the `dfs -text` command. For example, replace the `dfs -cat` command\nfrom above with the following to view files stored in `avro` format:\n\n    hdfs dfs -copyToLocal /user/flume/<event_dir>/<yyyy-mm-dd>/FlumeData.<id> /home/ubuntu/myFile.txt\n    hdfs dfs -text file:///home/ubuntu/myFile.txt\n\n\n## Contact Information\n\n- <bigdata@lists.ubuntu.com>\n\n\n## Help\n\n- [Apache Flume home page](http://flume.apache.org/)\n- [Apache Flume bug tracker](https://issues.apache.org/jira/browse/flume)\n- [Apache Flume mailing lists](https://flume.apache.org/mailinglists.html)\n- `#juju` on `irc.freenode.net`\n",
  "readme_name": "README.md",
  "gatherbase_origin": "juju-charmstore"
}