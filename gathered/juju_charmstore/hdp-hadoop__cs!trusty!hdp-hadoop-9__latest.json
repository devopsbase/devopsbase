{
  "name": "hdp-hadoop Juju charm",
  "juju_charm_name": "hdp-hadoop",
  "revision": "cs:trusty/hdp-hadoop-9",
  "latest": true,
  "uris": [
    "https://jujucharms.com/hdp-hadoop",
    "https://jujucharms.com/hdp-hadoop/trusty",
    "https://jujucharms.com/hdp-hadoop/trusty/9",
    "https://api.jujucharms.com/v5/hdp-hadoop",
    "https://api.jujucharms.com/v5/trusty/hdp-hadoop",
    "https://api.jujucharms.com/v5/trusty/hdp-hadoop-9"
  ],
  "labels": [
    "Juju charm",
    "applications",
    "Mode/Executable/Bundle/Juju Charm",
    "Style/Virtualization Level/Application",
    "Type/Devopsware/Deployment/Juju",
    "Type/Middleware/Data Processing/Hadoop"
  ],
  "info_url": "https://jujucharms.com/hdp-hadoop",
  "package_url": "https://api.jujucharms.com/v5/trusty/hdp-hadoop-9/archive",
  "created": "2015-06-17T09:34:57.874Z",
  "updated": "2015-06-17T09:34:57.874Z",
  "description": "Software platform for processing vast amounts of data\n\nHadoop is a software platform that lets one easily write and\nrun applications that process vast amounts of data.\n",
  "maintainer": {
    "name": "bigdata-charmers"
  },
  "juju_charm_subordinate": false,
  "juju_charm_series": "trusty",
  "juju_charm_owner": "bigdata-charmers",
  "requires": [
    {
      "kind": "host",
      "label": "Infrastructure/Operating System/Linux/Ubuntu",
      "version": "= trusty"
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/dfs",
      "self_resolve": true,
      "juju_interface": "dfs",
      "juju_name": "datanode",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/mapred",
      "self_resolve": true,
      "juju_interface": "mapred",
      "juju_name": "nodemanager",
      "juju_role": "requirer",
      "juju_limit": 1
    }
  ],
  "parameters": {
    "dfs_data_dir": {
      "type": "string",
      "description": "Space separated list of directories where DataNode will store file system image.",
      "default": "/grid/hadoop/hdfs/dn",
      "mapping": "charm_option"
    },
    "dfs_name_dir": {
      "type": "string",
      "description": "Space separated list of directories where NameNode will store file system image.",
      "default": "/grid/hadoop/hdfs/nn",
      "mapping": "charm_option"
    },
    "fs_checkpoint_dir": {
      "type": "string",
      "description": "Space separated list of directories where SecondaryNameNode will store checkpoint image.",
      "default": "/grid/hadoop/hdfs/snn",
      "mapping": "charm_option"
    },
    "yarn_local_dir": {
      "type": "string",
      "description": "Space separated list of directories where YARN will store temporary data..",
      "default": "/grid/hadoop/yarn/local",
      "mapping": "charm_option"
    },
    "yarn_local_log_dir": {
      "type": "string",
      "description": "Space separated list of directories where YARN will store container log data.",
      "default": "/grid/hadoop/yarn/local",
      "mapping": "charm_option"
    },
    "zookeeper_data_dir": {
      "type": "string",
      "description": "Directory where ZooKeeper will store data.",
      "default": "/grid/hadoop/zookeeper/data",
      "mapping": "charm_option"
    }
  },
  "provides": [
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/mapred",
      "juju_interface": "mapred",
      "juju_name": "hadoop-nodes",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/dfs",
      "juju_interface": "dfs",
      "juju_name": "namenode",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/mapred",
      "juju_interface": "mapred",
      "juju_name": "resourcemanager",
      "juju_role": "provider",
      "juju_limit": 0
    }
  ],
  "juju_peers": {
    "compute-nodes": {
      "Name": "compute-nodes",
      "Role": "peer",
      "Interface": "mapred",
      "Optional": false,
      "Limit": 1,
      "Scope": "global"
    }
  },
  "license": "## Overview\n**What is Hortonworks Apache Hadoop (HDP 2.1.3) ?**\nThe Apache Hadoop software library is a framework that allows for the\ndistributed processing of large data sets across clusters of computers\nusing a simple programming model.\n\nIt is designed to scale up from single servers to thousands of machines,each\noffering local computation and storage. Rather than rely on hardware to deliver\nhigh-avaiability, the library itself is designed to detect and handle failures\nat the application layer, so delivering a highly-availabile service on top of a\ncluster of computers, each of which may be prone to failures.\n\nApache Hadoop 2.4.1 consists of significant improvements over the previous\nstable release (hadoop-1.x).\n\nHere is a short overview of the improvments to both HDFS and MapReduce.\n- **HDFS Federation**\n  In order to scale the name service horizontally, federation uses multiple\n  independent Namenodes/Namespaces. The Namenodes are federated, that is, the\n  Namenodes are independent and don't require coordination with each other. The\n  datanodes are used as common storage for blocks by all the Namenodes. Each\n  datanode registers with all the Namenodes in the cluster. Datanodes send\n  periodic heartbeats and block reports and handles commands from the Namenodes.\n\n  More details are available in the [HDFS Federation document] (http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-hdfs/Federation.html)\n\n- **MapReduce NextGen aka YARN aka MRv2**\n  The new architecture introduced in hadoop-0.23, divides the two major\n  functions of the JobTracker: resource management and job life-cycle management\n  into separate components. The new ResourceManager manages the global\n  assignment of compute resources to applications and the per-application\n  ApplicationMaster manages the applicationâ€š scheduling and coordination.\n  An application is either a single job in the sense of classic MapReduce jobs\n  or a DAG of such jobs.\n\n  The ResourceManager and per-machine NodeManager daemon, which manages the\n  user processes of that machine, form the computation fabric.\n\n  The per-application ApplicationMaster is, in effect, a framework specific\n  library and is tasked with negotiating resources from the ResourceManager and\n  working with the NodeManager(s) to execute and monitor the tasks.\n\nMore details are available in the [YARN document](http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/YARN.html)\n\n## Usage\n\nThis charm supports the following Hadoop roles:\n\n- HDFS: namenode, secondarynamenode and datanode ( TBD HDFS Federation)\n- YARN: ResourceManager, NodeManager\n\nThis supports deployments of Hadoop in a number of configurations.\n\n### HDP 2.1.3 Usage #1: Combined HDFS and MapReduce\n\nIn this configuration, the YARN ResourceManager is deployed on the same\nservice units as HDFS namenode and the HDFS datanodes also run YARN NodeManager::\n    juju deploy hdp-hadoop yarn-hdfs-master\n    juju deploy hdp-hadoop compute-node\n    juju add-unit -n 2 compute-node\n    juju add-relation yarn-hdfs-master:namenode compute-node:datanode\n    juju add-relation yarn-hdfs-master:resourcemanager compute-node:nodemanager\n\n\n\n## Known Limitations and Issues\n\nNote that removing the relation between namenode and datanode is destructive!\nThe role of the service is determined at the point that the relation is added\n(it must be qualified) and CANNOT be changed later!\n\nA single hdfs-master can support multiple slave service deployments::\n\n    juju deploy hadoop hdfs-datacluster-02\n    juju add-unit -n 2 hdfs-datacluster-02\n    juju add-relation hdfs-namenode:namenode hdfs-datacluster-02:datanode\n\n\n\n# Contact Information\namir sanjar <amir.sanjar@canonical.com>\n\n## Hadoop\n\n- [Apache Hadoop](http://hadoop.apache.org/) home page\n- [Apache Hadoop bug trackers](http://hadoop.apache.org/issue_tracking.html)\n- [Apache Hadoop mailing lists](http://hadoop.apache.org/mailing_lists.html)\n- [Apache Hadoop Juju Charm](http://jujucharms.com/?text=hadoop)\n",
  "readme": "## Overview\n**What is Hortonworks Apache Hadoop (HDP 2.1.3) ?**\nThe Apache Hadoop software library is a framework that allows for the\ndistributed processing of large data sets across clusters of computers\nusing a simple programming model.\n\nIt is designed to scale up from single servers to thousands of machines,each\noffering local computation and storage. Rather than rely on hardware to deliver\nhigh-avaiability, the library itself is designed to detect and handle failures\nat the application layer, so delivering a highly-availabile service on top of a\ncluster of computers, each of which may be prone to failures.\n\nApache Hadoop 2.4.1 consists of significant improvements over the previous\nstable release (hadoop-1.x).\n\nHere is a short overview of the improvments to both HDFS and MapReduce.\n- **HDFS Federation**\n  In order to scale the name service horizontally, federation uses multiple\n  independent Namenodes/Namespaces. The Namenodes are federated, that is, the\n  Namenodes are independent and don't require coordination with each other. The\n  datanodes are used as common storage for blocks by all the Namenodes. Each\n  datanode registers with all the Namenodes in the cluster. Datanodes send\n  periodic heartbeats and block reports and handles commands from the Namenodes.\n\n  More details are available in the [HDFS Federation document] (http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-hdfs/Federation.html)\n\n- **MapReduce NextGen aka YARN aka MRv2**\n  The new architecture introduced in hadoop-0.23, divides the two major\n  functions of the JobTracker: resource management and job life-cycle management\n  into separate components. The new ResourceManager manages the global\n  assignment of compute resources to applications and the per-application\n  ApplicationMaster manages the applicationâ€š scheduling and coordination.\n  An application is either a single job in the sense of classic MapReduce jobs\n  or a DAG of such jobs.\n\n  The ResourceManager and per-machine NodeManager daemon, which manages the\n  user processes of that machine, form the computation fabric.\n\n  The per-application ApplicationMaster is, in effect, a framework specific\n  library and is tasked with negotiating resources from the ResourceManager and\n  working with the NodeManager(s) to execute and monitor the tasks.\n\nMore details are available in the [YARN document](http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/YARN.html)\n\n## Usage\n\nThis charm supports the following Hadoop roles:\n\n- HDFS: namenode, secondarynamenode and datanode ( TBD HDFS Federation)\n- YARN: ResourceManager, NodeManager\n\nThis supports deployments of Hadoop in a number of configurations.\n\n### HDP 2.1.3 Usage #1: Combined HDFS and MapReduce\n\nIn this configuration, the YARN ResourceManager is deployed on the same\nservice units as HDFS namenode and the HDFS datanodes also run YARN NodeManager::\n    juju deploy hdp-hadoop yarn-hdfs-master\n    juju deploy hdp-hadoop compute-node\n    juju add-unit -n 2 compute-node\n    juju add-relation yarn-hdfs-master:namenode compute-node:datanode\n    juju add-relation yarn-hdfs-master:resourcemanager compute-node:nodemanager\n\n\n\n## Known Limitations and Issues\n\nNote that removing the relation between namenode and datanode is destructive!\nThe role of the service is determined at the point that the relation is added\n(it must be qualified) and CANNOT be changed later!\n\nA single hdfs-master can support multiple slave service deployments::\n\n    juju deploy hadoop hdfs-datacluster-02\n    juju add-unit -n 2 hdfs-datacluster-02\n    juju add-relation hdfs-namenode:namenode hdfs-datacluster-02:datanode\n\n\n\n# Contact Information\namir sanjar <amir.sanjar@canonical.com>\n\n## Hadoop\n\n- [Apache Hadoop](http://hadoop.apache.org/) home page\n- [Apache Hadoop bug trackers](http://hadoop.apache.org/issue_tracking.html)\n- [Apache Hadoop mailing lists](http://hadoop.apache.org/mailing_lists.html)\n- [Apache Hadoop Juju Charm](http://jujucharms.com/?text=hadoop)\n",
  "readme_name": "README.md",
  "gatherbase_origin": "juju-charmstore"
}