{
  "name": "hbase Juju charm",
  "juju_charm_name": "hbase",
  "revision": "cs:precise/hbase-8",
  "latest": true,
  "uris": [
    "https://jujucharms.com/hbase",
    "https://jujucharms.com/hbase/precise",
    "https://jujucharms.com/hbase/precise/8",
    "https://api.jujucharms.com/v5/hbase",
    "https://api.jujucharms.com/v5/precise/hbase",
    "https://api.jujucharms.com/v5/precise/hbase-8"
  ],
  "labels": [
    "Juju charm",
    "databases",
    "Binding/Region/Europe/EU",
    "Binding/Region/North America/US",
    "Mode/Executable/Bundle/Juju Charm",
    "Style/Virtualization Level/Hardware",
    "Type/Devopsware/Deployment/Juju",
    "Type/Devopsware/Orchestration/Cluster-based",
    "Type/Infrastructure/Operating System",
    "Type/Middleware/Data Store/Document-oriented/HBase",
    "Type/Middleware/Data Store/Column",
    "Type/Middleware/Data Processing/Hadoop"
  ],
  "info_url": "https://jujucharms.com/hbase",
  "package_url": "https://api.jujucharms.com/v5/precise/hbase-8/archive",
  "created": "2015-06-17T09:37:40.478Z",
  "updated": "2015-06-17T09:37:40.478Z",
  "description": "The Hadoop dataBASE\n\nHBase is the Hadoop dataBASE\n.\nUse it when you need random, realtime read/write access to your Big Data.\nThis project's goal is the hosting of very large tables -- billions of rows\nX millions of columns -- atop clusters of commodity hardware.\n",
  "maintainer": {
    "name": "charmers"
  },
  "juju_charm_subordinate": false,
  "juju_charm_series": "precise",
  "juju_charm_owner": "charmers",
  "requires": [
    {
      "kind": "host",
      "label": "Infrastructure/Operating System/Linux/Ubuntu",
      "version": "= precise"
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/mapred",
      "self_resolve": true,
      "juju_interface": "mapred",
      "juju_name": "jobtracker",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/dfs",
      "self_resolve": true,
      "juju_interface": "dfs",
      "juju_name": "namenode",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/hbase",
      "self_resolve": true,
      "juju_interface": "hbase",
      "juju_name": "regionserver",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/zookeeper",
      "self_resolve": true,
      "juju_interface": "zookeeper",
      "juju_name": "zookeeper",
      "juju_role": "requirer",
      "juju_limit": 1
    }
  ],
  "parameters": {
    "heap": {
      "type": "int",
      "description": "The maximum heap size in MB to allocate for daemons processes within the\nservice units managed by this charm.\n.\nThe recommended configurations vary based on role:\n.\n * Master: 4GB.\n * RegionServer: 12GB, or the majority of available memory.\n.\nThe above recommendations are taken from HBase: The Definitive Guide by\nLars George.\n.\nObviously you need to ensure that the servers supporting each service unit\nhave sufficient memory to accomodate this setting - it should be no more\nthan 75% of the total memory in the system excluding swap.\n",
      "default": 1024,
      "mapping": "charm_option"
    },
    "pig": {
      "type": "boolean",
      "description": "To install Apache Pig on all service units alongside Hadoop set this\nconfiguration to 'True'.\n.\nApache Pig is a platform for analyzing large data sets that consists\nof a high-level language for expressing data analysis programs, coupled\nwith infrastructure for evaluating these programs. The salient property\nof Pig programs is that their structure is amenable to substantial\nparallelization, which in turns enables them to handle very large data\nsets.\n",
      "default": false,
      "mapping": "charm_option"
    },
    "source": {
      "type": "string",
      "description": "Location and packages to install hbase:\n.\n * dev:     Install using the hbase packages from\n            ppa:hadoop-ubuntu/dev.\n * testing: Install using the hbase packages from\n            ppa:hadoop-ubuntu/testing.\n * stable:  Install using the hbase packages from\n            ppa:hadoop-ubuntu/stable.\n.\nThe packages provided in the hadoop-ubuntu team PPA's are based\ndirectly on upstream hbase releases but are not fully built from\nsource.\n",
      "default": "stable",
      "mapping": "charm_option"
    }
  },
  "provides": [
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/http",
      "juju_interface": "http",
      "juju_name": "avro",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/hbase",
      "juju_interface": "hbase",
      "juju_name": "master",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/http",
      "juju_interface": "http",
      "juju_name": "rest",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/http",
      "juju_interface": "http",
      "juju_name": "thrift",
      "juju_role": "provider",
      "juju_limit": 0
    }
  ],
  "juju_peers": {
    "root-ssh": {
      "Name": "root-ssh",
      "Role": "peer",
      "Interface": "ssh",
      "Optional": false,
      "Limit": 1,
      "Scope": "global"
    }
  },
  "license": "# HBase Overview\n\nHBase is the Hadoop database. Think of it as a distributed scalable Big Data\nstore.\n\nUse HBase when you need random, realtime read/write access to your Big Data.\nThis project's goal is the hosting of very large tables -- billions of rows X\nmillions of columns -- atop clusters of commodity hardware.\n\nHBase is an open-source, distributed, versioned, column-oriented store modeled\nafter Google's Bigtable: A Distributed Storage System for Structured Data by\nChang et al. Just as Bigtable leverages the distributed data storage provided\nby the Google File System, HBase provides Bigtable-like capabilities on top of\nHadoop and HDFS.\n\nHBase provides:\n\n- Linear and modular scalability.\n- Strictly consistent reads and writes.\n- Automatic and configurable sharding of tables\n- Automatic failover support between RegionServers.\n- Convenient base classes for backing Hadoop MapReduce jobs with HBase tables.\n- Easy to use Java API for client access.\n- Block cache and Bloom Filters for real-time queries.\n- Query predicate push down via server side Filters\n- Thrift gateway and a REST-ful Web service that supports XML, Protobuf,\n  and binary data encoding options\n- Extensible jruby-based (JIRB) shell\n- Support for exporting metrics via the Hadoop metrics subsystem to files\n  or Ganglia; or via JMX.\n\nSee [the homepage](http://hbase.apache.org) for more information.\n\nThis charm provides the hbase master and regionserver roles which form\npart of an overall hbase deployment\n\n## Usage\n\n\nA HBase deployment consists of a HBase master service and one or more\nHBase RegionServer services::\n\n    juju deploy hbase hbase-master\n    juju deploy hbase hbase-regioncluster-01\n\nIn order to function correctly the hbase master and regionserver services\nhave a mandatory relationship with zookeeper - please use the zookeeper charm\nto create a functional zookeeper quorum and then relate it to this charm::\n\n    juju deploy zookeeper hbase-zookeeper\n    juju add-units -n 2 hbase-zookeeper\n    juju add-relation hbase-master hbase-zookeeper\n    juju add-relation hbase-regioncluster-01 hbase-zookeeper\n\nRemember that quorums come in odd numbers start from 3 (but it will work\nwith one BUT with no resilience).\n\nThe hbase services also require the services of an hdfs namenode; these are\nprovided by the hadoop charm.\n\nHBase requires that append mode is enabled in DFS - this can be set by providing\na config.yaml file::\n\n    hdfs-namenode:\n        hbase: True\n    hdfs-datacluster-01:\n        hbase: True\n\nIts really important to ensure that both the master and the slave services have\nthe same configuration in this deployment scenario::\n\n    juju deploy --config config.yaml hadoop hdfs-namenode\n    juju deploy --config config.yaml hadoop hdfs-datacluster-01\n    juju add-relation hdfs-namenode:namenode hdfs-datacluster-01:datanode\n\nThe hadoop services can also support mapreduce - please see the hadoop charm\nfor more details.\n\nThe namenode can then be related to the hbase deployment::\n\n    juju add-relation hdfs-namenode:namenode hbase-master:namenode\n    juju add-relation hdfs-namenode:namenode hbase-regioncluster-01:namenode\n\nOnce the hbase services have been related to both zookeeper and hdfs they\ncan be related to each other::\n\n    juju add-relation hbase-master:master hbase-regioncluster-01:regionserver\n\nAt this point the role of each service is fixed and CANNOT be changed. ever.\nperiod.\n\nIts also possible to run with more that one hbase master service unit::\n\n    juju add-unit hbase-master\n\nThe masters will coordinate through zookeeper to establish control of the\ncluster and will re-coordinate if one of the master service units disappears.\n\nYou can also add additional regionservers::\n\n    juju add-unit -n 2 hbase-regioncluster-01\n\nThe charm also supports use of the thrift, avro and rest gateways.  Any hbase\nservice can be used in this way by associating another service with it::\n\n    juju add-relation hush:thrift hbase-regioncluster-01:thrift\n\nOR you can deploy a seperate gateway server::\n\n    juju deploy hbase hbase-thrift\n    juju add-relation hbase-thrift hbase-zookeeper\n    juju add-relation hush:thrift hbase-thrift:thrift\n\nthrift, avro and rest all operate over HTTP and are stateless so use with\nhaproxy is possible::\n\n    juju deploy haproxy rest-gateway\n    juju add-relation rest-gateway hbase-regioncluster-01:rest\n\n\n## Rolling Restarts\n\nRestarting a HBase deployment is potentially disruptive so the charm will NOT\nautomatically restart HBase when the following events occur:\n\n- Zookeeper service units joining or departing relations.\n- Upgrading the charm or changing the configuration.\n\nHowever the charm will update configuration files and automatically sets up\nSSH key authentication between nodes within a service deployment and from the\nmaster service to regionserver services.\n\nA rolling restart script is provided by the charm which will restart you HBase\ndeployment in a controlled fashion::\n\n    juju ssh hbase-master/0 hbase-rolling-restart\n\nIf any inconsistencies are found in HBase the restart will not happen. The script\nalso supports just restarting regionservers::\n\n    juju ssh hbase-master/0 hbase-rolling-restart --rs-only\n\nor just masters::\n  \n    juju ssh hbase-master/0 hbase-rolling-restart --master-only\n\nThis script must be run from a HBase master.\n",
  "readme": "# HBase Overview\n\nHBase is the Hadoop database. Think of it as a distributed scalable Big Data\nstore.\n\nUse HBase when you need random, realtime read/write access to your Big Data.\nThis project's goal is the hosting of very large tables -- billions of rows X\nmillions of columns -- atop clusters of commodity hardware.\n\nHBase is an open-source, distributed, versioned, column-oriented store modeled\nafter Google's Bigtable: A Distributed Storage System for Structured Data by\nChang et al. Just as Bigtable leverages the distributed data storage provided\nby the Google File System, HBase provides Bigtable-like capabilities on top of\nHadoop and HDFS.\n\nHBase provides:\n\n- Linear and modular scalability.\n- Strictly consistent reads and writes.\n- Automatic and configurable sharding of tables\n- Automatic failover support between RegionServers.\n- Convenient base classes for backing Hadoop MapReduce jobs with HBase tables.\n- Easy to use Java API for client access.\n- Block cache and Bloom Filters for real-time queries.\n- Query predicate push down via server side Filters\n- Thrift gateway and a REST-ful Web service that supports XML, Protobuf,\n  and binary data encoding options\n- Extensible jruby-based (JIRB) shell\n- Support for exporting metrics via the Hadoop metrics subsystem to files\n  or Ganglia; or via JMX.\n\nSee [the homepage](http://hbase.apache.org) for more information.\n\nThis charm provides the hbase master and regionserver roles which form\npart of an overall hbase deployment\n\n## Usage\n\n\nA HBase deployment consists of a HBase master service and one or more\nHBase RegionServer services::\n\n    juju deploy hbase hbase-master\n    juju deploy hbase hbase-regioncluster-01\n\nIn order to function correctly the hbase master and regionserver services\nhave a mandatory relationship with zookeeper - please use the zookeeper charm\nto create a functional zookeeper quorum and then relate it to this charm::\n\n    juju deploy zookeeper hbase-zookeeper\n    juju add-units -n 2 hbase-zookeeper\n    juju add-relation hbase-master hbase-zookeeper\n    juju add-relation hbase-regioncluster-01 hbase-zookeeper\n\nRemember that quorums come in odd numbers start from 3 (but it will work\nwith one BUT with no resilience).\n\nThe hbase services also require the services of an hdfs namenode; these are\nprovided by the hadoop charm.\n\nHBase requires that append mode is enabled in DFS - this can be set by providing\na config.yaml file::\n\n    hdfs-namenode:\n        hbase: True\n    hdfs-datacluster-01:\n        hbase: True\n\nIts really important to ensure that both the master and the slave services have\nthe same configuration in this deployment scenario::\n\n    juju deploy --config config.yaml hadoop hdfs-namenode\n    juju deploy --config config.yaml hadoop hdfs-datacluster-01\n    juju add-relation hdfs-namenode:namenode hdfs-datacluster-01:datanode\n\nThe hadoop services can also support mapreduce - please see the hadoop charm\nfor more details.\n\nThe namenode can then be related to the hbase deployment::\n\n    juju add-relation hdfs-namenode:namenode hbase-master:namenode\n    juju add-relation hdfs-namenode:namenode hbase-regioncluster-01:namenode\n\nOnce the hbase services have been related to both zookeeper and hdfs they\ncan be related to each other::\n\n    juju add-relation hbase-master:master hbase-regioncluster-01:regionserver\n\nAt this point the role of each service is fixed and CANNOT be changed. ever.\nperiod.\n\nIts also possible to run with more that one hbase master service unit::\n\n    juju add-unit hbase-master\n\nThe masters will coordinate through zookeeper to establish control of the\ncluster and will re-coordinate if one of the master service units disappears.\n\nYou can also add additional regionservers::\n\n    juju add-unit -n 2 hbase-regioncluster-01\n\nThe charm also supports use of the thrift, avro and rest gateways.  Any hbase\nservice can be used in this way by associating another service with it::\n\n    juju add-relation hush:thrift hbase-regioncluster-01:thrift\n\nOR you can deploy a seperate gateway server::\n\n    juju deploy hbase hbase-thrift\n    juju add-relation hbase-thrift hbase-zookeeper\n    juju add-relation hush:thrift hbase-thrift:thrift\n\nthrift, avro and rest all operate over HTTP and are stateless so use with\nhaproxy is possible::\n\n    juju deploy haproxy rest-gateway\n    juju add-relation rest-gateway hbase-regioncluster-01:rest\n\n\n## Rolling Restarts\n\nRestarting a HBase deployment is potentially disruptive so the charm will NOT\nautomatically restart HBase when the following events occur:\n\n- Zookeeper service units joining or departing relations.\n- Upgrading the charm or changing the configuration.\n\nHowever the charm will update configuration files and automatically sets up\nSSH key authentication between nodes within a service deployment and from the\nmaster service to regionserver services.\n\nA rolling restart script is provided by the charm which will restart you HBase\ndeployment in a controlled fashion::\n\n    juju ssh hbase-master/0 hbase-rolling-restart\n\nIf any inconsistencies are found in HBase the restart will not happen. The script\nalso supports just restarting regionservers::\n\n    juju ssh hbase-master/0 hbase-rolling-restart --rs-only\n\nor just masters::\n  \n    juju ssh hbase-master/0 hbase-rolling-restart --master-only\n\nThis script must be run from a HBase master.\n",
  "readme_name": "README.md",
  "gatherbase_origin": "juju-charmstore"
}