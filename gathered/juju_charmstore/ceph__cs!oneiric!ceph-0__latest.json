{
  "name": "ceph Juju charm",
  "juju_charm_name": "ceph",
  "revision": "cs:oneiric/ceph-0",
  "latest": true,
  "uris": [
    "https://jujucharms.com/ceph",
    "https://jujucharms.com/ceph/oneiric",
    "https://jujucharms.com/ceph/oneiric/0",
    "https://api.jujucharms.com/v5/ceph",
    "https://api.jujucharms.com/v5/oneiric/ceph",
    "https://api.jujucharms.com/v5/oneiric/ceph-0"
  ],
  "labels": [
    "Juju charm",
    "Type/Middleware/Runtime/Java",
    "Binding/Region/North America/US",
    "Mode/Executable/Bundle/Juju Charm",
    "Style/Virtualization Level/Hardware/Network",
    "Type/Devopsware/Deployment/Juju",
    "Type/Devopsware/Orchestration/Cluster-based",
    "Type/Middleware/Data Store"
  ],
  "info_url": "https://jujucharms.com/ceph",
  "package_url": "https://api.jujucharms.com/v5/oneiric/ceph-0/archive",
  "created": "2015-06-17T09:33:20.524Z",
  "updated": "2015-06-17T09:33:20.524Z",
  "description": "distributed storage and file system\n\nCeph is a distributed storage and network file system designed to\nprovide excellent performance, reliability, and scalability.  This\npackage contains all server daemons and management tools for creating,\nrunning, and administering a Ceph storage cluster.\n",
  "maintainer": {
    "name": "charmers"
  },
  "juju_charm_subordinate": false,
  "juju_charm_series": "oneiric",
  "juju_charm_owner": "charmers",
  "requires": [
    {
      "kind": "host",
      "label": "Infrastructure/Operating System/Linux/Ubuntu",
      "version": "= oneiric"
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/ceph-mds",
      "self_resolve": true,
      "juju_interface": "ceph-mds",
      "juju_name": "mds-remote",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/ceph-mon",
      "self_resolve": true,
      "juju_interface": "ceph-mon",
      "juju_name": "mon-remote",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/ceph-osd",
      "self_resolve": true,
      "juju_interface": "ceph-osd",
      "juju_name": "osd-remote",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/root-ssh",
      "self_resolve": true,
      "juju_interface": "root-ssh",
      "juju_name": "ssh-client",
      "juju_role": "requirer",
      "juju_limit": 1
    }
  ],
  "parameters": {
    "osd-journal-size": {
      "type": "int",
      "description": "Size of each node's OSD journal in Megabytes",
      "default": 512,
      "mapping": "charm_option"
    },
    "rados-port": {
      "type": "int",
      "description": "What port to listen for radosgw requests on. 0 means do\nnot setup a radosgw on this service.\n",
      "default": 0,
      "mapping": "charm_option"
    },
    "root-ssh": {
      "type": "string",
      "description": "Allow all nodes to ssh as root to all other nodes. This\nsounds a bit risky, but its needed to mkcephfs, so only\nturn it on while doing mkcephfs, then turn it back off.\n",
      "default": "yes",
      "mapping": "charm_option"
    },
    "run-mds": {
      "type": "string",
      "description": "Set to \"yes\" to run all members of this service as metadata serverss\n",
      "default": "yes",
      "mapping": "charm_option"
    },
    "run-mon": {
      "type": "string",
      "description": "Set to \"yes\" to run all members of this service as monitors\n",
      "default": "yes",
      "mapping": "charm_option"
    },
    "run-osd": {
      "type": "string",
      "description": "Set to \"yes\" to run all members of this service as OSDs\n",
      "default": "yes",
      "mapping": "charm_option"
    }
  },
  "provides": [
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/ceph-mds",
      "juju_interface": "ceph-mds",
      "juju_name": "mds-server",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/ceph-mon",
      "juju_interface": "ceph-mon",
      "juju_name": "mon-server",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/ceph-osd",
      "juju_interface": "ceph-osd",
      "juju_name": "osd-server",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/root-ssh",
      "juju_interface": "root-ssh",
      "juju_name": "ssh-remote",
      "juju_role": "provider",
      "juju_limit": 0
    }
  ],
  "juju_peers": {
    "mds": {
      "Name": "mds",
      "Role": "peer",
      "Interface": "ceph-mds",
      "Optional": false,
      "Limit": 1,
      "Scope": "global"
    },
    "mon": {
      "Name": "mon",
      "Role": "peer",
      "Interface": "ceph-mon",
      "Optional": false,
      "Limit": 1,
      "Scope": "global"
    },
    "osd": {
      "Name": "osd",
      "Role": "peer",
      "Interface": "ceph-osd",
      "Optional": false,
      "Limit": 1,
      "Scope": "global"
    },
    "ssh": {
      "Name": "ssh",
      "Role": "peer",
      "Interface": "root-ssh",
      "Optional": false,
      "Limit": 1,
      "Scope": "global"
    }
  },
  "license": "This is a rudimentary start at a workable elastic ceph charm.\n\nTo deploy, simply deploy it as a service, and add units. All nodes will\nrun all components of CEPH currently (mds, osd, and mon).  This means\nyou should not try to use the cluster until it has reached an odd number\nof machines.\n\nBecause I haven't worked out how to do mkcephfs properly:\n\nFirst deploy on one node. SSH to it, and run:\n\nsudo mkcephfs -a -c /etc/ceph/ceph.conf\n\nIt should create a ceph filesystem with data stored on /mnt.  On EC2\ninstances, this is automatically a large ephemeral drive with ext4,\nand should perform reasonably well.\n\nAfter this, add-unit to grow/shrink the cluster. Use the 'run-xxx'\nflags and remote-(mds|osd|mon) to relate one service with another.\n\nWhen you are not adding units, its probably best to disable the root\nssh with:\n\njuju set name-of-service root-ssh=no\n\nOnce done, one should be able to mount the ceph filesystem using any of\nthe service unit IP's.\n\nAfter you are done, you can improve security by turning off the root ssh,\nwhich is only used for mkcephfs, with:\n\n",
  "readme": "This is a rudimentary start at a workable elastic ceph charm.\n\nTo deploy, simply deploy it as a service, and add units. All nodes will\nrun all components of CEPH currently (mds, osd, and mon).  This means\nyou should not try to use the cluster until it has reached an odd number\nof machines.\n\nBecause I haven't worked out how to do mkcephfs properly:\n\nFirst deploy on one node. SSH to it, and run:\n\nsudo mkcephfs -a -c /etc/ceph/ceph.conf\n\nIt should create a ceph filesystem with data stored on /mnt.  On EC2\ninstances, this is automatically a large ephemeral drive with ext4,\nand should perform reasonably well.\n\nAfter this, add-unit to grow/shrink the cluster. Use the 'run-xxx'\nflags and remote-(mds|osd|mon) to relate one service with another.\n\nWhen you are not adding units, its probably best to disable the root\nssh with:\n\njuju set name-of-service root-ssh=no\n\nOnce done, one should be able to mount the ceph filesystem using any of\nthe service unit IP's.\n\nAfter you are done, you can improve security by turning off the root ssh,\nwhich is only used for mkcephfs, with:\n\n",
  "readme_name": "README.md",
  "gatherbase_origin": "juju-charmstore"
}