{
  "name": "hdp-hive Juju charm",
  "juju_charm_name": "hdp-hive",
  "revision": "cs:trusty/hdp-hive-5",
  "latest": true,
  "uris": [
    "https://jujucharms.com/hdp-hive",
    "https://jujucharms.com/hdp-hive/trusty",
    "https://jujucharms.com/hdp-hive/trusty/5",
    "https://api.jujucharms.com/v5/hdp-hive",
    "https://api.jujucharms.com/v5/trusty/hdp-hive",
    "https://api.jujucharms.com/v5/trusty/hdp-hive-5"
  ],
  "labels": [
    "Juju charm",
    "databases",
    "Binding/Region/North America/US",
    "Mode/Executable/Image/VM Image/AMI",
    "Mode/Executable/Bundle/Juju Charm",
    "Type/Devopsware/Deployment/Juju",
    "Type/Infrastructure",
    "Type/Middleware/Data Store",
    "Type/Middleware/Data Processing/Hadoop/Hive"
  ],
  "info_url": "https://jujucharms.com/hdp-hive",
  "package_url": "https://api.jujucharms.com/v5/trusty/hdp-hive-5/archive",
  "created": "2015-06-17T09:33:31.446Z",
  "updated": "2015-06-17T09:33:31.446Z",
  "description": "Data warehouse infrastructure built on top of Hadoop\n\nData warehouse infrastructure built on top of Hadoop\n.\nHive is a data warehouse infrastructure built on top of Hadoop that\nprovides tools to enable easy data summarization, adhoc querying and\nanalysis of large datasets data stored in Hadoop files. It provides a\nmechanism to put structure on this data and it also provides a simple\nquery language called Hive QL which is based on SQL and which enables\nusers familiar with SQL to query this data. At the same time, this\nlanguage also allows traditional map/reduce programmers to be able to\nplug in their custom mappers and reducers to do more sophisticated\nanalysis which may not be supported by the built-in capabilities of\nthe language.\n",
  "maintainer": {
    "name": "bigdata-charmers"
  },
  "juju_charm_subordinate": false,
  "juju_charm_series": "trusty",
  "juju_charm_owner": "bigdata-charmers",
  "requires": [
    {
      "kind": "host",
      "label": "Infrastructure/Operating System/Linux/Ubuntu",
      "version": "= trusty"
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/mysql",
      "self_resolve": true,
      "juju_interface": "mysql",
      "juju_name": "db",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/elasticsearch",
      "self_resolve": true,
      "juju_interface": "elasticsearch",
      "juju_name": "elk",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/hive",
      "self_resolve": true,
      "juju_interface": "hive",
      "juju_name": "metastore",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/dfs",
      "self_resolve": true,
      "juju_interface": "dfs",
      "juju_name": "namenode",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/mapred",
      "self_resolve": true,
      "juju_interface": "mapred",
      "juju_name": "resourcemanager",
      "juju_role": "requirer",
      "juju_limit": 1
    }
  ],
  "parameters": {
    "heap": {
      "type": "int",
      "description": "The maximum heap size in MB to allocate for daemons processes within the\nservice units managed by this charm.\n",
      "default": 1024,
      "mapping": "charm_option"
    }
  },
  "provides": [
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/hive",
      "juju_interface": "hive",
      "juju_name": "server",
      "juju_role": "provider",
      "juju_limit": 0
    }
  ],
  "license": "# Hortonworks HIVE Overview\nData warehouse infrastructure built on top of Hortonwork Apache HIVE.\n \nHortonworks Apache Hive 0.12.x is a data warehouse infrastructure built \non top of Hortonworks Hadoop 2.4.1 that\nprovides tools to enable easy data summarization, adhoc querying and\nanalysis of large datasets data stored in Hadoop files. It provides a\nmechanism to put structure on this data and it also provides a simple\nquery language called Hive QL which is based on SQL and which enables\nusers familiar with SQL to query this data. At the same time, this\nlanguage also allows traditional map/reduce programmers to be able to\nplug in their custom mappers and reducers to do more sophisticated\nanalysis which may not be supported by the built-in capabilities of\nthe language.\n\nHive provides:\n\n- HiveQL - An SQL dialect language for querying data in a RDBMS fashion\n- UDF/UDAF/UDTF (User Defined [Aggregate/Table] Functions) - Allows user to\n  create custom Map/Reduce based functions for regular use\n- Ability to do joins (inner/outer/semi) between tables\n- Support (limited) for sub-queries\n- Support for table 'Views'\n- Ability to partition data into Hive partitions or buckets to enable faster\n  querying\n- Hive Web Interface - A web interface to Hive\n- Hive Server2 - Supports multi-suer querying using Thrift, JDBC and ODBC clients\n- Hive Metastore - Ability to run a separate Metadata storage process\n-* Hive cli - A Hive commandline that supports HiveQL\n\nSee [http://hive.apache.org]http://hive.apache.org) for more information.\n\nThis charm provides the Hive Server and Metastore roles which form part of an\noverall Hive deployment.\n\n# Hortonworks HIVE Usage\n\nA Hive deployment consists of a Hive service, a RDBMS (only MySQL is currently\nsupported), an optional Metastore service and a Hadoop cluster.\n\nTo deploy a simple four node Hadoop cluster (see Hadoop charm README for further\ninformation)::\n    juju deploy hdp-hadoop yarn-hdfs-master\n    juju deploy hdp-hadoop compute-node\n    juju add-unit -n 2 compute-node\n    juju add-relation yarn-hdfs-master:namenode compute-node:datanode\n    juju add-relation yarn-hdfs-master:resourcemanager compute-node:nodemanager\n    \nA Hive server stores metadata in MySQL::\n\n    juju deploy mysql\n    # hive requires ROW binlog\n    juju set mysql binlog-format=ROW\n\nTo deploy a Hive service without a Metastore service::\n\n    # deploy Hive instance (hive-server2)\n    juju deploy hdp-hive hdphive \n    # associate Hive with MySQL\n    juju add-relation hdphive:db mysql:db\n\n    # associate Hive with HDFS Namenode\n    juju add-relation hdphive:namenode yarn-hdfs-master:namenode\n    # associate Hive with resourcemanager\n    juju add-relation hdphive:resourcemanager yarn-hdfs-master:resourcemanager\n    \n# Smoke Test\n## Usage\nOnce you have a cluster running, just run:\n    1) juju ssh yarn-hdfs-master/0  <<= ssh to hadoop master\n    2) Smoke test HDFS admin functionality- As the HDFS user, create a /user/$CLIENT_USER in\n       hadoop file system - Below steps verifies/demos HDFS functionality  \n       a) sudo su $HDFS_USER\n       b) hdfs dfs -mkdir -p /user/ubuntu\n       c) hdfs dfs -chown ubuntu:ubuntu /user/ubuntu\n       d) hdfs dfs -chmod -R 755 /user/ubuntu\n       e) exit\n\n    3) Smoke test YARN and Mapreduce - Run the smoke test as the $CLIENT_USER, \n       using Terasort and sort 10GB of data.\n       a) hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples-*.jar \n          teragen 10000 /user/ubuntu/teragenout \n       b) hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples-*.jar \n          terasort /user/ubuntu/teragenout /user/ubuntu/terasortout\n\n    4) Smoke test HDFS funtionality from ubuntu user space - delete mapreduce \n       output from hdfs \n       hdfs dfs -rm -r /user/ubuntu/teragenout\n\n    HIVE+HDFS Usage:\n    1) juju ssh hdphive/0  <<= ssh to hive server\n    2) sudo su $HIVE_USER\n    3) hive\n    4) from Hive console:\n       show databases;\n       create table test(col1 int, col2 string);\n       show tables;\n       exit;\n    5) exit from $HIVE_USER session\n    6) sudo su $HDFS_USER\n    7) hadoop dfsadmin -report <<== verify connection to the remote HDFS cluster\n    8) hdfs dfs -ls /apps/hive/warehouse <<== verify that \"test\" directory has \n    been created on the remote HDFS cluster \n    \n##Scale Out Usage\nIn order to increase the amount of slaves, you must add units, to add one unit:\n    juju add-unit compute-node\nOr you can add multiple units at once:\n    juju add-unit -n4 compute-node\n# Contact Information\namir sanjar <amir.sanjar@canonical.com>\n\n## Upstream Project Name\n\n- Upstream website http://hortonworks.com/\n- Upstream bug tracker https://issues.apache.org/jira/browse/HIVE\n",
  "readme": "# Hortonworks HIVE Overview\nData warehouse infrastructure built on top of Hortonwork Apache HIVE.\n \nHortonworks Apache Hive 0.12.x is a data warehouse infrastructure built \non top of Hortonworks Hadoop 2.4.1 that\nprovides tools to enable easy data summarization, adhoc querying and\nanalysis of large datasets data stored in Hadoop files. It provides a\nmechanism to put structure on this data and it also provides a simple\nquery language called Hive QL which is based on SQL and which enables\nusers familiar with SQL to query this data. At the same time, this\nlanguage also allows traditional map/reduce programmers to be able to\nplug in their custom mappers and reducers to do more sophisticated\nanalysis which may not be supported by the built-in capabilities of\nthe language.\n\nHive provides:\n\n- HiveQL - An SQL dialect language for querying data in a RDBMS fashion\n- UDF/UDAF/UDTF (User Defined [Aggregate/Table] Functions) - Allows user to\n  create custom Map/Reduce based functions for regular use\n- Ability to do joins (inner/outer/semi) between tables\n- Support (limited) for sub-queries\n- Support for table 'Views'\n- Ability to partition data into Hive partitions or buckets to enable faster\n  querying\n- Hive Web Interface - A web interface to Hive\n- Hive Server2 - Supports multi-suer querying using Thrift, JDBC and ODBC clients\n- Hive Metastore - Ability to run a separate Metadata storage process\n-* Hive cli - A Hive commandline that supports HiveQL\n\nSee [http://hive.apache.org]http://hive.apache.org) for more information.\n\nThis charm provides the Hive Server and Metastore roles which form part of an\noverall Hive deployment.\n\n# Hortonworks HIVE Usage\n\nA Hive deployment consists of a Hive service, a RDBMS (only MySQL is currently\nsupported), an optional Metastore service and a Hadoop cluster.\n\nTo deploy a simple four node Hadoop cluster (see Hadoop charm README for further\ninformation)::\n    juju deploy hdp-hadoop yarn-hdfs-master\n    juju deploy hdp-hadoop compute-node\n    juju add-unit -n 2 compute-node\n    juju add-relation yarn-hdfs-master:namenode compute-node:datanode\n    juju add-relation yarn-hdfs-master:resourcemanager compute-node:nodemanager\n    \nA Hive server stores metadata in MySQL::\n\n    juju deploy mysql\n    # hive requires ROW binlog\n    juju set mysql binlog-format=ROW\n\nTo deploy a Hive service without a Metastore service::\n\n    # deploy Hive instance (hive-server2)\n    juju deploy hdp-hive hdphive \n    # associate Hive with MySQL\n    juju add-relation hdphive:db mysql:db\n\n    # associate Hive with HDFS Namenode\n    juju add-relation hdphive:namenode yarn-hdfs-master:namenode\n    # associate Hive with resourcemanager\n    juju add-relation hdphive:resourcemanager yarn-hdfs-master:resourcemanager\n    \n# Smoke Test\n## Usage\nOnce you have a cluster running, just run:\n    1) juju ssh yarn-hdfs-master/0  <<= ssh to hadoop master\n    2) Smoke test HDFS admin functionality- As the HDFS user, create a /user/$CLIENT_USER in\n       hadoop file system - Below steps verifies/demos HDFS functionality  \n       a) sudo su $HDFS_USER\n       b) hdfs dfs -mkdir -p /user/ubuntu\n       c) hdfs dfs -chown ubuntu:ubuntu /user/ubuntu\n       d) hdfs dfs -chmod -R 755 /user/ubuntu\n       e) exit\n\n    3) Smoke test YARN and Mapreduce - Run the smoke test as the $CLIENT_USER, \n       using Terasort and sort 10GB of data.\n       a) hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples-*.jar \n          teragen 10000 /user/ubuntu/teragenout \n       b) hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples-*.jar \n          terasort /user/ubuntu/teragenout /user/ubuntu/terasortout\n\n    4) Smoke test HDFS funtionality from ubuntu user space - delete mapreduce \n       output from hdfs \n       hdfs dfs -rm -r /user/ubuntu/teragenout\n\n    HIVE+HDFS Usage:\n    1) juju ssh hdphive/0  <<= ssh to hive server\n    2) sudo su $HIVE_USER\n    3) hive\n    4) from Hive console:\n       show databases;\n       create table test(col1 int, col2 string);\n       show tables;\n       exit;\n    5) exit from $HIVE_USER session\n    6) sudo su $HDFS_USER\n    7) hadoop dfsadmin -report <<== verify connection to the remote HDFS cluster\n    8) hdfs dfs -ls /apps/hive/warehouse <<== verify that \"test\" directory has \n    been created on the remote HDFS cluster \n    \n##Scale Out Usage\nIn order to increase the amount of slaves, you must add units, to add one unit:\n    juju add-unit compute-node\nOr you can add multiple units at once:\n    juju add-unit -n4 compute-node\n# Contact Information\namir sanjar <amir.sanjar@canonical.com>\n\n## Upstream Project Name\n\n- Upstream website http://hortonworks.com/\n- Upstream bug tracker https://issues.apache.org/jira/browse/HIVE\n",
  "readme_name": "README.md",
  "gatherbase_origin": "juju-charmstore"
}