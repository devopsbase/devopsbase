{
  "name": "apache-spark Juju charm",
  "juju_charm_name": "apache-spark",
  "revision": "cs:apache-spark-13",
  "latest": true,
  "uris": [
    "https://jujucharms.com/apache-spark",
    "https://jujucharms.com/apache-spark/xenial",
    "https://jujucharms.com/apache-spark/xenial/13",
    "https://api.jujucharms.com/v5/apache-spark",
    "https://api.jujucharms.com/v5/xenial/apache-spark",
    "https://api.jujucharms.com/v5/xenial/apache-spark-13"
  ],
  "labels": [
    "Juju charm",
    "bigdata",
    "hadoop",
    "apache",
    "Mode/Executable/Bundle/Juju Charm",
    "Type/Devopsware/Deployment/Juju",
    "Type/Middleware/Web Server/Apache HTTP Server",
    "Type/Middleware/Data Processing"
  ],
  "info_url": "https://jujucharms.com/apache-spark",
  "package_url": "https://api.jujucharms.com/v5/xenial/apache-spark-13/archive",
  "created": "2016-09-23T17:22:57.761Z",
  "updated": "2016-09-23T17:22:57.761Z",
  "description": "Apache Spark is a fast engine for large-scale data processing\n\nApache Spark is a fast and general engine for large-scale data processing.\n\nLearn more at http://spark.apache.org.\n",
  "maintainer": {
    "name": "bigdata-charmers"
  },
  "juju_charm_subordinate": false,
  "juju_charm_series": "xenial",
  "juju_charm_owner": "bigdata-charmers",
  "requires": [
    {
      "kind": "host",
      "label": "Infrastructure/Operating System/Linux/Ubuntu",
      "version": "= xenial"
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/zookeeper",
      "self_resolve": true,
      "juju_interface": "zookeeper",
      "juju_name": "zookeeper",
      "juju_role": "requirer",
      "juju_limit": 1
    }
  ],
  "parameters": {
    "driver_memory": {
      "type": "string",
      "description": "Specify gigabytes (e.g. 1g) or megabytes (e.g. 1024m). If running\nin 'local' or 'standalone' mode, you may also specify a percentage\nof total system memory (e.g. 50%).\n",
      "default": "1g",
      "mapping": "charm_option"
    },
    "executor_memory": {
      "type": "string",
      "description": "Specify gigabytes (e.g. 1g) or megabytes (e.g. 1024m). If running\nin 'local' or 'standalone' mode, you may also specify a percentage\nof total system memory (e.g. 50%).\n",
      "default": "1g",
      "mapping": "charm_option"
    },
    "resources_mirror": {
      "type": "string",
      "description": "URL used to fetch resources (e.g., Hadoop binaries) instead of the\nlocation specified in resources.yaml.\n",
      "default": "",
      "mapping": "charm_option"
    },
    "spark_bench_enabled": {
      "type": "boolean",
      "description": "When set to 'true' (the default), this charm will download and\ninstall the SparkBench benchmark suite from the configured URLs.\nWhen set to 'false', SparkBench will be removed from the unit,\nthough any data stored in hdfs:///user/ubuntu/spark-bench will be\npreserved.\n",
      "default": true,
      "mapping": "charm_option"
    },
    "spark_bench_ppc64le": {
      "type": "string",
      "description": "URL (including hash) of a ppc64le tarball of SparkBench. By\ndefault, this points to a pre-built SparkBench binary based on\nsources in the upstream repository. This option is only valid when\n'spark_bench_enabled' is 'true'.\n",
      "default": "https://s3.amazonaws.com/jujubigdata/ibm/noarch/spark-bench-2.0-20151214-ffb72f23.tgz#sha256=ffb72f233eaafccef4dda6d4516f23e043d1b14b9d63734211f4d1968db86a3c",
      "mapping": "charm_option"
    },
    "spark_bench_x86_64": {
      "type": "string",
      "description": "URL (including hash) of an x86_64 tarball of SparkBench. By\ndefault, this points to a pre-built SparkBench binary based on\nsources in the upstream repository. This option is only valid when\n'spark_bench_enabled' is 'true'.\n",
      "default": "https://s3.amazonaws.com/jujubigdata/ibm/noarch/spark-bench-2.0-20151214-ffb72f23.tgz#sha256=ffb72f233eaafccef4dda6d4516f23e043d1b14b9d63734211f4d1968db86a3c",
      "mapping": "charm_option"
    },
    "spark_execution_mode": {
      "type": "string",
      "description": "Options are \"local\", \"standalone\", \"yarn-client\", and\n\"yarn-cluster\". Consult the readme for details on these options.\n",
      "default": "standalone",
      "mapping": "charm_option"
    },
    "spark_version": {
      "type": "string",
      "description": "Version of Apache Spark to be deployed.\n",
      "default": "1.6.1-hadoop2.6.0",
      "mapping": "charm_option"
    }
  },
  "provides": [
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/benchmark",
      "juju_interface": "benchmark",
      "juju_name": "benchmark",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/spark",
      "juju_interface": "spark",
      "juju_name": "client",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/hadoop-plugin",
      "juju_interface": "hadoop-plugin",
      "juju_name": "hadoop",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/java",
      "juju_interface": "java",
      "juju_name": "java",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/mahout",
      "juju_interface": "mahout",
      "juju_name": "mahout",
      "juju_role": "provider",
      "juju_limit": 0
    }
  ],
  "juju_peers": {
    "sparkpeers": {
      "Name": "sparkpeers",
      "Role": "peer",
      "Interface": "spark-quorum",
      "Optional": false,
      "Limit": 1,
      "Scope": "global"
    }
  },
  "license": "## Overview\n\n### Spark Cluster\n\nApache Spark™ is a fast and general purpose engine for large-scale data\nprocessing. Key features:\n\n * **Speed**\n\n Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster\n on disk. Spark has an advanced DAG execution engine that supports cyclic data\n flow and in-memory computing.\n\n * **Ease of Use**\n\n Write applications quickly in Java, Scala or Python. Spark offers over 80\n high-level operators that make it easy to build parallel apps, and you can use\n it interactively from the Scala and Python shells.\n\n * **General Purpose Engine**\n\n Combine SQL, streaming, and complex analytics. Spark powers a stack of\n high-level tools including Shark for SQL, MLlib for machine learning, GraphX,\n and Spark Streaming. You can combine these frameworks seamlessly in the same\n application.\n\n\n## Deployment\n\nThis charm allows the deployment of Apache Spark in the modes described below:\n\n * **Standalone**\n\n In this mode Spark units form a cluster that you can scale to match your needs.\n Starting with a single node:\n\n    juju deploy apache-spark spark\n\n You can scale the cluster by adding more spark units:\n\n    juju add-unit spark\n\n When in standalone mode Juju ensures a single Spark master is appointed.\n The status of the unit acting as master reads \"Ready (standalone - master)\",\n while the rest of the units display a status of  \"Ready (standalone)\".\n In case you remove the master unit Juju will appoint a new master to the cluster.\n However, should a master fail in this standalone mode the cluster will stop functioning\n properly. Master node failures is handled properly when Apache spark is setup in\n High Availability mode (Standalone HA).\n\n * **Standalone HA**\n\n To enable High Availability properties of a cluster you need to add a relation\n between spark and a zookeeper deployment. The suggested deployment method is to use the\n [apache-spark-HA](https://jujucharms.com/apache-spark-ha/) bundle.\n\n    juju-quickstart apache-spark-ha\n\n In this mode again you can scale your cluster to match your needs by adding/removing\n units. Spark units report \"Ready (standalone HA)\" in their status so if you need to\n identify the node acting as master you need to query the Zookeeper deployment.\n\n    juju ssh zk/0\n    zkCli.sh\n    get /spark/master_status\n\n * **Yarn-client and Yarn-cluster**\n\n This charm leverages our pluggable Hadoop model with the `hadoop-plugin`\n interface. This means that you can relate this charm to a base Apache Hadoop cluster\n to run Spark jobs there. The suggested deployment method is to use the\n [apache-hadoop-spark](https://jujucharms.com/apache-hadoop-spark/)\n bundle. This will deploy the Apache Hadoop platform with a single Apache Spark\n unit that communicates with the cluster by relating to the\n `apache-hadoop-plugin` subordinate charm:\n\n    juju-quickstart apache-hadoop-spark\n\n\nNote: To transition among execution modes you need to set the\n`spark-execution-mode` config variable:\n\n    juju set spark spark-execution-mode=<new_mode>\n\n## Usage\n\nOnce deployment is complete, you can manually load and run Spark batch or\nstreaming jobs in a variety of ways:\n\n  * **Spark shell**\n\nSpark’s shell provides a simple way to learn the API, as well as a powerful\ntool to analyse data interactively. It is available in either Scala or Python\nand can be run from the Spark unit as follows:\n\n       juju ssh spark/0\n       spark-shell # for interaction using scala\n       pyspark     # for interaction using python\n\n  * **Command line**\n\nSSH to the Spark unit and manually run a spark-submit job, for example:\n\n       juju ssh spark/0\n       spark-submit --class org.apache.spark.examples.SparkPi \\\n        --master yarn-client /usr/lib/spark/lib/spark-examples*.jar 10\n\n  * **Apache Zeppelin visual service**\n\nDeploy Apache Zeppelin and relate it to the Spark unit:\n\n    juju deploy apache-zeppelin zeppelin\n    juju add-relation spark zeppelin\n\nOnce the relation has been made, access the web interface at\n`http://{spark_unit_ip_address}:9090`\n\n  * **IPyNotebook for Spark**\n\nThe IPython Notebook is an interactive computational environment, in which you\ncan combine code execution, rich text, mathematics, plots and rich media.\nDeploy IPython Notebook for Spark and relate it to the Spark unit:\n\n    juju deploy apache-spark-notebook notebook\n    juju add-relation spark notebook\n\nOnce the relation has been made, access the web interface at\n`http://{spark_unit_ip_address}:8880`\n\n\n## Configuration\n\n### `driver_memory`\n\nAmount of memory Spark will request for the Master. Specify gigabytes (e.g.\n1g) or megabytes (e.g. 1024m). If running in `local` or `standalone` mode, you\nmay also specify a percentage of total system memory (e.g. 50%).\n\n### `executor_memory`\n\nAmount of memory Spark will request for each executor. Specify gigabytes (e.g.\n1g) or megabytes (e.g. 1024m). If running in `local` or `standalone` mode, you\nmay also specify a percentage of total system memory (e.g. 50%). Take care\nwhen specifying percentages in local modes, as this value is for *each*\nexecutor. Your Spark job will fail if, for example, you set this value > 50%\nand attempt to run 2 or more executors.\n\n### `spark_bench_enabled`\n\nInstall the SparkBench benchmarking suite. If `true` (the default), this charm\nwill download spark bench from the URL specified by `spark_bench_ppc64le`\nor `spark_bench_x86_64`, depending on the unit's architecture.\n\n### `spark-execution-mode`\n\nSpark has four modes of execution: local, standalone, yarn-client, and\nyarn-cluster. The default mode is `yarn-client` and can be changed by setting\nthe `spark_execution_mode` config variable.\n\n  * **Local**\n\n    In Local mode, Spark processes jobs locally without any cluster resources.\n    There are 3 ways to specify 'local' mode:\n\n    * `local`\n\n      Run Spark locally with one worker thread (i.e. no parallelism at all).\n\n    * `local[K]`\n\n      Run Spark locally with K worker threads (ideally, set this to the number\n      of cores on your machine).\n\n    * `local[*]`\n\n      Run Spark locally with as many worker threads as logical cores on your\n      machine.\n\n  * **Standalone**\n\n    In `standalone` mode, Spark launches a Master and Worker daemon on the Spark\n    unit. This mode is useful for simulating a distributed cluster environment\n    without actually setting up a cluster.\n\n  * **YARN-client**\n\n    In `yarn-client` mode, the driver runs in the client process, and the\n    application master is only used for requesting resources from YARN.\n\n  * **YARN-cluster**\n\n    In `yarn-cluster` mode, the Spark driver runs inside an application master\n    process which is managed by YARN on the cluster, and the client can go away\n    after initiating the application.\n\n\n## Verify the deployment\n\n### Status and Smoke Test\n\nThe services provide extended status reporting to indicate when they are ready:\n\n    juju status --format=tabular\n\nThis is particularly useful when combined with `watch` to track the on-going\nprogress of the deployment:\n\n    watch -n 0.5 juju status --format=tabular\n\nThe message for each unit will provide information about that unit's state.\nOnce they all indicate that they are ready, you can perform a \"smoke test\"\nto verify that Spark is working as expected using the built-in `smoke-test`\naction:\n\n    juju action do spark/0 smoke-test\n\nAfter a few seconds or so, you can check the results of the smoke test:\n\n    juju action status\n\nYou will see `status: completed` if the smoke test was successful, or\n`status: failed` if it was not.  You can get more information on why it failed\nvia:\n\n    juju action fetch <action-id>\n\n\n### Verify Job History\n\nThe Job History server shows all active and finished spark jobs submitted.\nTo view the Job History server you need to expose spark (`juju expose spark`)\nand navigate to `http://{spark_master_unit_ip_address}:18080` of the\nunit acting as master.\n\n\n## Benchmarking\n\nThis charm provides several benchmarks, including the\n[Spark Bench](https://github.com/SparkTC/spark-bench) benchmarking\nsuite (if enabled), to gauge the performance of your environment.\n\nThe easiest way to run the benchmarks on this service is to relate it to the\n[Benchmark GUI][].  You will likely also want to relate it to the\n[Benchmark Collector][] to have machine-level information collected during the\nbenchmark, for a more complete picture of how the machine performed.\n\n[Benchmark GUI]: https://jujucharms.com/benchmark-gui/\n[Benchmark Collector]: https://jujucharms.com/benchmark-collector/\n\nHowever, each benchmark is also an action that can be called manually:\n\n    $ juju action do spark/0 pagerank\n    Action queued with id: 88de9367-45a8-4a4b-835b-7660f467a45e\n    $ juju action fetch --wait 0 88de9367-45a8-4a4b-835b-7660f467a45e\n    results:\n      meta:\n        composite:\n          direction: asc\n          units: secs\n          value: \"77.939000\"\n        raw: |\n          PageRank,2015-12-10-23:41:57,77.939000,71.888079,.922363,0,PageRank-MLlibConfig,,,,,10,12,,200000,4.0,1.3,0.15\n        start: 2015-12-10T23:41:34Z\n        stop: 2015-12-10T23:43:16Z\n      results:\n        duration:\n          direction: asc\n          units: secs\n          value: \"77.939000\"\n        throughput:\n          direction: desc\n          units: x/sec\n          value: \".922363\"\n    status: completed\n    timing:\n      completed: 2015-12-10 23:43:59 +0000 UTC\n      enqueued: 2015-12-10 23:42:10 +0000 UTC\n      started: 2015-12-10 23:42:15 +0000 UTC\n\nValid action names at this time are:\n\n  * logisticregression\n  * matrixfactorization\n  * pagerank\n  * sql\n  * streaming\n  * svdplusplus\n  * svm\n  * trianglecount\n  * sparkpi\n\n\n## Upgrading and Apache Bigtop Spark\n\nThis charm does not currently support upgrading, because it currently only\nsupports a single version of Apache Spark.  It is recommended to look to the\n[Apache Bigtop Spark](https://jujucharms.com/apache-bigtop-spark/) charm for\nsupport of future versions of Apache Spark, via the Apache Bigtop project.\n\n## Contact Information\n\n- <bigdata@lists.ubuntu.com>\n\n\n## Help\n\n- [Juju mailing list](https://lists.ubuntu.com/mailman/listinfo/juju)\n- [Juju community](https://jujucharms.com/community)\n",
  "readme": "## Overview\n\n### Spark Cluster\n\nApache Spark™ is a fast and general purpose engine for large-scale data\nprocessing. Key features:\n\n * **Speed**\n\n Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster\n on disk. Spark has an advanced DAG execution engine that supports cyclic data\n flow and in-memory computing.\n\n * **Ease of Use**\n\n Write applications quickly in Java, Scala or Python. Spark offers over 80\n high-level operators that make it easy to build parallel apps, and you can use\n it interactively from the Scala and Python shells.\n\n * **General Purpose Engine**\n\n Combine SQL, streaming, and complex analytics. Spark powers a stack of\n high-level tools including Shark for SQL, MLlib for machine learning, GraphX,\n and Spark Streaming. You can combine these frameworks seamlessly in the same\n application.\n\n\n## Deployment\n\nThis charm allows the deployment of Apache Spark in the modes described below:\n\n * **Standalone**\n\n In this mode Spark units form a cluster that you can scale to match your needs.\n Starting with a single node:\n\n    juju deploy apache-spark spark\n\n You can scale the cluster by adding more spark units:\n\n    juju add-unit spark\n\n When in standalone mode Juju ensures a single Spark master is appointed.\n The status of the unit acting as master reads \"Ready (standalone - master)\",\n while the rest of the units display a status of  \"Ready (standalone)\".\n In case you remove the master unit Juju will appoint a new master to the cluster.\n However, should a master fail in this standalone mode the cluster will stop functioning\n properly. Master node failures is handled properly when Apache spark is setup in\n High Availability mode (Standalone HA).\n\n * **Standalone HA**\n\n To enable High Availability properties of a cluster you need to add a relation\n between spark and a zookeeper deployment. The suggested deployment method is to use the\n [apache-spark-HA](https://jujucharms.com/apache-spark-ha/) bundle.\n\n    juju-quickstart apache-spark-ha\n\n In this mode again you can scale your cluster to match your needs by adding/removing\n units. Spark units report \"Ready (standalone HA)\" in their status so if you need to\n identify the node acting as master you need to query the Zookeeper deployment.\n\n    juju ssh zk/0\n    zkCli.sh\n    get /spark/master_status\n\n * **Yarn-client and Yarn-cluster**\n\n This charm leverages our pluggable Hadoop model with the `hadoop-plugin`\n interface. This means that you can relate this charm to a base Apache Hadoop cluster\n to run Spark jobs there. The suggested deployment method is to use the\n [apache-hadoop-spark](https://jujucharms.com/apache-hadoop-spark/)\n bundle. This will deploy the Apache Hadoop platform with a single Apache Spark\n unit that communicates with the cluster by relating to the\n `apache-hadoop-plugin` subordinate charm:\n\n    juju-quickstart apache-hadoop-spark\n\n\nNote: To transition among execution modes you need to set the\n`spark-execution-mode` config variable:\n\n    juju set spark spark-execution-mode=<new_mode>\n\n## Usage\n\nOnce deployment is complete, you can manually load and run Spark batch or\nstreaming jobs in a variety of ways:\n\n  * **Spark shell**\n\nSpark’s shell provides a simple way to learn the API, as well as a powerful\ntool to analyse data interactively. It is available in either Scala or Python\nand can be run from the Spark unit as follows:\n\n       juju ssh spark/0\n       spark-shell # for interaction using scala\n       pyspark     # for interaction using python\n\n  * **Command line**\n\nSSH to the Spark unit and manually run a spark-submit job, for example:\n\n       juju ssh spark/0\n       spark-submit --class org.apache.spark.examples.SparkPi \\\n        --master yarn-client /usr/lib/spark/lib/spark-examples*.jar 10\n\n  * **Apache Zeppelin visual service**\n\nDeploy Apache Zeppelin and relate it to the Spark unit:\n\n    juju deploy apache-zeppelin zeppelin\n    juju add-relation spark zeppelin\n\nOnce the relation has been made, access the web interface at\n`http://{spark_unit_ip_address}:9090`\n\n  * **IPyNotebook for Spark**\n\nThe IPython Notebook is an interactive computational environment, in which you\ncan combine code execution, rich text, mathematics, plots and rich media.\nDeploy IPython Notebook for Spark and relate it to the Spark unit:\n\n    juju deploy apache-spark-notebook notebook\n    juju add-relation spark notebook\n\nOnce the relation has been made, access the web interface at\n`http://{spark_unit_ip_address}:8880`\n\n\n## Configuration\n\n### `driver_memory`\n\nAmount of memory Spark will request for the Master. Specify gigabytes (e.g.\n1g) or megabytes (e.g. 1024m). If running in `local` or `standalone` mode, you\nmay also specify a percentage of total system memory (e.g. 50%).\n\n### `executor_memory`\n\nAmount of memory Spark will request for each executor. Specify gigabytes (e.g.\n1g) or megabytes (e.g. 1024m). If running in `local` or `standalone` mode, you\nmay also specify a percentage of total system memory (e.g. 50%). Take care\nwhen specifying percentages in local modes, as this value is for *each*\nexecutor. Your Spark job will fail if, for example, you set this value > 50%\nand attempt to run 2 or more executors.\n\n### `spark_bench_enabled`\n\nInstall the SparkBench benchmarking suite. If `true` (the default), this charm\nwill download spark bench from the URL specified by `spark_bench_ppc64le`\nor `spark_bench_x86_64`, depending on the unit's architecture.\n\n### `spark-execution-mode`\n\nSpark has four modes of execution: local, standalone, yarn-client, and\nyarn-cluster. The default mode is `yarn-client` and can be changed by setting\nthe `spark_execution_mode` config variable.\n\n  * **Local**\n\n    In Local mode, Spark processes jobs locally without any cluster resources.\n    There are 3 ways to specify 'local' mode:\n\n    * `local`\n\n      Run Spark locally with one worker thread (i.e. no parallelism at all).\n\n    * `local[K]`\n\n      Run Spark locally with K worker threads (ideally, set this to the number\n      of cores on your machine).\n\n    * `local[*]`\n\n      Run Spark locally with as many worker threads as logical cores on your\n      machine.\n\n  * **Standalone**\n\n    In `standalone` mode, Spark launches a Master and Worker daemon on the Spark\n    unit. This mode is useful for simulating a distributed cluster environment\n    without actually setting up a cluster.\n\n  * **YARN-client**\n\n    In `yarn-client` mode, the driver runs in the client process, and the\n    application master is only used for requesting resources from YARN.\n\n  * **YARN-cluster**\n\n    In `yarn-cluster` mode, the Spark driver runs inside an application master\n    process which is managed by YARN on the cluster, and the client can go away\n    after initiating the application.\n\n\n## Verify the deployment\n\n### Status and Smoke Test\n\nThe services provide extended status reporting to indicate when they are ready:\n\n    juju status --format=tabular\n\nThis is particularly useful when combined with `watch` to track the on-going\nprogress of the deployment:\n\n    watch -n 0.5 juju status --format=tabular\n\nThe message for each unit will provide information about that unit's state.\nOnce they all indicate that they are ready, you can perform a \"smoke test\"\nto verify that Spark is working as expected using the built-in `smoke-test`\naction:\n\n    juju action do spark/0 smoke-test\n\nAfter a few seconds or so, you can check the results of the smoke test:\n\n    juju action status\n\nYou will see `status: completed` if the smoke test was successful, or\n`status: failed` if it was not.  You can get more information on why it failed\nvia:\n\n    juju action fetch <action-id>\n\n\n### Verify Job History\n\nThe Job History server shows all active and finished spark jobs submitted.\nTo view the Job History server you need to expose spark (`juju expose spark`)\nand navigate to `http://{spark_master_unit_ip_address}:18080` of the\nunit acting as master.\n\n\n## Benchmarking\n\nThis charm provides several benchmarks, including the\n[Spark Bench](https://github.com/SparkTC/spark-bench) benchmarking\nsuite (if enabled), to gauge the performance of your environment.\n\nThe easiest way to run the benchmarks on this service is to relate it to the\n[Benchmark GUI][].  You will likely also want to relate it to the\n[Benchmark Collector][] to have machine-level information collected during the\nbenchmark, for a more complete picture of how the machine performed.\n\n[Benchmark GUI]: https://jujucharms.com/benchmark-gui/\n[Benchmark Collector]: https://jujucharms.com/benchmark-collector/\n\nHowever, each benchmark is also an action that can be called manually:\n\n    $ juju action do spark/0 pagerank\n    Action queued with id: 88de9367-45a8-4a4b-835b-7660f467a45e\n    $ juju action fetch --wait 0 88de9367-45a8-4a4b-835b-7660f467a45e\n    results:\n      meta:\n        composite:\n          direction: asc\n          units: secs\n          value: \"77.939000\"\n        raw: |\n          PageRank,2015-12-10-23:41:57,77.939000,71.888079,.922363,0,PageRank-MLlibConfig,,,,,10,12,,200000,4.0,1.3,0.15\n        start: 2015-12-10T23:41:34Z\n        stop: 2015-12-10T23:43:16Z\n      results:\n        duration:\n          direction: asc\n          units: secs\n          value: \"77.939000\"\n        throughput:\n          direction: desc\n          units: x/sec\n          value: \".922363\"\n    status: completed\n    timing:\n      completed: 2015-12-10 23:43:59 +0000 UTC\n      enqueued: 2015-12-10 23:42:10 +0000 UTC\n      started: 2015-12-10 23:42:15 +0000 UTC\n\nValid action names at this time are:\n\n  * logisticregression\n  * matrixfactorization\n  * pagerank\n  * sql\n  * streaming\n  * svdplusplus\n  * svm\n  * trianglecount\n  * sparkpi\n\n\n## Upgrading and Apache Bigtop Spark\n\nThis charm does not currently support upgrading, because it currently only\nsupports a single version of Apache Spark.  It is recommended to look to the\n[Apache Bigtop Spark](https://jujucharms.com/apache-bigtop-spark/) charm for\nsupport of future versions of Apache Spark, via the Apache Bigtop project.\n\n## Contact Information\n\n- <bigdata@lists.ubuntu.com>\n\n\n## Help\n\n- [Juju mailing list](https://lists.ubuntu.com/mailman/listinfo/juju)\n- [Juju community](https://jujucharms.com/community)\n",
  "readme_name": "README.md",
  "gatherbase_origin": "juju-charmstore"
}