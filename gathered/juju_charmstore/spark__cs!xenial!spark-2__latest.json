{
  "name": "spark Juju charm",
  "juju_charm_name": "spark",
  "revision": "cs:xenial/spark-2",
  "latest": true,
  "uris": [
    "https://jujucharms.com/spark",
    "https://jujucharms.com/spark/xenial",
    "https://jujucharms.com/spark/xenial/2",
    "https://api.jujucharms.com/v5/spark",
    "https://api.jujucharms.com/v5/xenial/spark",
    "https://api.jujucharms.com/v5/xenial/spark-2"
  ],
  "labels": [
    "Juju charm",
    "apache",
    "bigdata",
    "bigtop",
    "hadoop",
    "applications",
    "Mode/Executable/Bundle/Juju Charm",
    "Type/Devopsware/Deployment/Juju",
    "Type/Middleware/Web Server/Apache HTTP Server",
    "Type/Middleware/Data Processing"
  ],
  "info_url": "https://jujucharms.com/spark",
  "package_url": "https://api.jujucharms.com/v5/xenial/spark-2/archive",
  "created": "2016-09-21T22:20:08.012Z",
  "updated": "2016-09-21T22:20:08.012Z",
  "description": "Apache Spark from Apache Bigtop platform\n\nApache Spark is a fast and general engine for large-scale data processing.\nLearn more at http://spark.apache.org.\n",
  "maintainer": {
    "name": "bigdata-charmers"
  },
  "juju_charm_subordinate": false,
  "juju_charm_series": "xenial",
  "juju_charm_owner": "bigdata-charmers",
  "requires": [
    {
      "kind": "host",
      "label": "Infrastructure/Operating System/Linux/Ubuntu",
      "version": "= xenial"
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/zookeeper",
      "self_resolve": true,
      "juju_interface": "zookeeper",
      "juju_name": "zookeeper",
      "juju_role": "requirer",
      "juju_limit": 1
    }
  ],
  "parameters": {
    "resources_mirror": {
      "type": "string",
      "description": "URL used to fetch resources (e.g., Hadoop binaries) instead of the\nlocation specified in resources.yaml.\n",
      "default": "",
      "mapping": "charm_option"
    },
    "spark_bench_enabled": {
      "type": "boolean",
      "description": "When set to 'true' (the default), this charm will download and\ninstall the SparkBench benchmark suite from the configured URLs.\nWhen set to 'false', SparkBench will be removed from the unit,\nthough any data stored in hdfs:///user/ubuntu/spark-bench will be\npreserved.\n",
      "default": true,
      "mapping": "charm_option"
    },
    "spark_bench_ppc64le": {
      "type": "string",
      "description": "URL (including hash) of a ppc64le tarball of SparkBench. By\ndefault, this points to a pre-built SparkBench binary based on\nsources in the upstream repository. This option is only valid when\n'spark_bench_enabled' is 'true'.\n",
      "default": "https://s3.amazonaws.com/jujubigdata/ibm/noarch/spark-bench-2.0-20151214-ffb72f23.tgz#sha256=ffb72f233eaafccef4dda6d4516f23e043d1b14b9d63734211f4d1968db86a3c",
      "mapping": "charm_option"
    },
    "spark_bench_x86_64": {
      "type": "string",
      "description": "URL (including hash) of an x86_64 tarball of SparkBench. By\ndefault, this points to a pre-built SparkBench binary based on\nsources in the upstream repository. This option is only valid when\n'spark_bench_enabled' is 'true'.\n",
      "default": "https://s3.amazonaws.com/jujubigdata/ibm/noarch/spark-bench-2.0-20151214-ffb72f23.tgz#sha256=ffb72f233eaafccef4dda6d4516f23e043d1b14b9d63734211f4d1968db86a3c",
      "mapping": "charm_option"
    },
    "spark_execution_mode": {
      "type": "string",
      "description": "Options are \"local\", \"standalone\", \"yarn-client\", and\n\"yarn-cluster\". Consult the readme for details on these options.\n",
      "default": "standalone",
      "mapping": "charm_option"
    }
  },
  "provides": [
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/benchmark",
      "juju_interface": "benchmark",
      "juju_name": "benchmark",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/spark",
      "juju_interface": "spark",
      "juju_name": "client",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/hadoop-plugin",
      "juju_interface": "hadoop-plugin",
      "juju_name": "hadoop",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/java",
      "juju_interface": "java",
      "juju_name": "java",
      "juju_role": "provider",
      "juju_limit": 0
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/mahout",
      "juju_interface": "mahout",
      "juju_name": "mahout",
      "juju_role": "provider",
      "juju_limit": 0
    }
  ],
  "juju_peers": {
    "sparkpeers": {
      "Name": "sparkpeers",
      "Role": "peer",
      "Interface": "spark-quorum",
      "Optional": false,
      "Limit": 1,
      "Scope": "global"
    }
  },
  "license": "<!--\n  Licensed to the Apache Software Foundation (ASF) under one or more\n  contributor license agreements.  See the NOTICE file distributed with\n  this work for additional information regarding copyright ownership.\n  The ASF licenses this file to You under the Apache License, Version 2.0\n  (the \"License\"); you may not use this file except in compliance with\n  the License.  You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n-->\n## Overview\n\nApache Spark™ is a fast and general purpose engine for large-scale data\nprocessing. Key features:\n\n * **Speed**\n\n Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster\n on disk. Spark has an advanced DAG execution engine that supports cyclic data\n flow and in-memory computing.\n\n * **Ease of Use**\n\n Write applications quickly in Java, Scala or Python. Spark offers over 80\n high-level operators that make it easy to build parallel apps, and you can use\n it interactively from the Scala and Python shells.\n\n * **General Purpose Engine**\n\n Combine SQL, streaming, and complex analytics. Spark powers a stack of\n high-level tools including Shark for SQL, MLlib for machine learning, GraphX,\n and Spark Streaming. You can combine these frameworks seamlessly in the same\n application.\n\n\n## Deployment\n\nThis charm deploys the Spark component of the Apache Bigtop platform and\nsupports running Spark in a variety of modes:\n\n * **Standalone**\n\n In this mode Spark units form a cluster that you can scale to match your needs.\n Starting with a single node:\n\n    juju deploy spark\n    juju deploy openjdk\n    juju add-relation spark openjdk\n\n You can scale the cluster by adding more spark units:\n\n    juju add-unit spark\n\n When in standalone mode, Juju ensures a single Spark master is appointed.\n The status of the unit acting as master reads \"ready (standalone - master)\",\n while the rest of the units display a status of \"ready (standalone)\".\n If you remove the master, Juju will appoint a new one. However, if a master\n fails in standalone mode, running jobs and job history will be lost.\n\n * **Standalone HA**\n\n To enable High Availability for a Spark cluster, you need to add Zookeeper to\n the deployment. To ensure a Zookeeper quorum, it is recommended that you\n deploy 3 units of the zookeeper application. For instance:\n\n    juju deploy apache-zookeeper zookeeper -n 3\n    juju add-relation spark zookeeper\n\n In this mode, you can again scale your cluster to match your needs by\n adding/removing units. Spark units report \"ready (standalone HA)\" in their\n status. If you need to identify the node acting as master, query Zookeeper\n as follows:\n\n    juju run --unit zookeeper/0 'echo \"get /spark/master_status\" | /usr/lib/zookeeper/bin/zkCli.sh'\n\n * **Yarn-client and Yarn-cluster**\n\n This charm leverages our pluggable Hadoop model with the `hadoop-plugin`\n interface. This means that you can relate this charm to a base Apache Hadoop cluster\n to run Spark jobs there. The suggested deployment method is to use the\n [hadoop-processing](https://jujucharms.com/hadoop-processing/)\n bundle and add a relation between spark and the plugin:\n\n    juju deploy hadoop-processing\n    juju add-relation plugin spark\n\n\nNote: To switch to a different execution mode, set the\n`spark_execution_mode` config variable:\n\n    juju set spark spark_execution_mode=<new_mode>\n\nSee the **Configuration** section below for supported mode options.\n\n\n## Usage\n\nOnce deployment is complete, you can manually load and run Spark batch or\nstreaming jobs in a variety of ways:\n\n  * **Spark shell**\n\nSpark’s shell provides a simple way to learn the API, as well as a powerful\ntool to analyse data interactively. It is available in either Scala or Python\nand can be run from the Spark unit as follows:\n\n       juju ssh spark/0\n       spark-shell # for interaction using scala\n       pyspark     # for interaction using python\n\n  * **Command line**\n\nSSH to the Spark unit and manually run a spark-submit job, for example:\n\n       juju ssh spark/0\n       spark-submit --class org.apache.spark.examples.SparkPi \\\n        --master yarn-client /usr/lib/spark/lib/spark-examples*.jar 10\n\n  * **Apache Zeppelin visual service**\n\nDeploy Apache Zeppelin and relate it to the Spark unit:\n\n    juju deploy apache-zeppelin zeppelin\n    juju add-relation spark zeppelin\n\nOnce the relation has been made, access the web interface at\n`http://{spark_unit_ip_address}:9090`\n\n  * **IPyNotebook for Spark**\n\nThe IPython Notebook is an interactive computational environment, in which you\ncan combine code execution, rich text, mathematics, plots and rich media.\nDeploy IPython Notebook for Spark and relate it to the Spark unit:\n\n    juju deploy apache-spark-notebook notebook\n    juju add-relation spark notebook\n\nOnce the relation has been made, access the web interface at\n`http://{spark_unit_ip_address}:8880`\n\n\n## Configuration\n\n### `spark_bench_enabled`\n\nInstall the SparkBench benchmarking suite. If `true` (the default), this charm\nwill download spark bench from the URL specified by `spark_bench_ppc64le`\nor `spark_bench_x86_64`, depending on the unit's architecture.\n\n### `spark_execution_mode`\n\nSpark has four modes of execution: local, standalone, yarn-client, and\nyarn-cluster. The default mode is `yarn-client` and can be changed by setting\nthe `spark_execution_mode` config variable.\n\n  * **Local**\n\n    In Local mode, Spark processes jobs locally without any cluster resources.\n    There are 3 ways to specify 'local' mode:\n\n    * `local`\n\n      Run Spark locally with one worker thread (i.e. no parallelism at all).\n\n    * `local[K]`\n\n      Run Spark locally with K worker threads (ideally, set this to the number\n      of cores on your machine).\n\n    * `local[*]`\n\n      Run Spark locally with as many worker threads as logical cores on your\n      machine.\n\n  * **Standalone**\n\n    In `standalone` mode, Spark launches a Master and Worker daemon on the Spark\n    unit. This mode is useful for simulating a distributed cluster environment\n    without actually setting up a cluster.\n\n  * **YARN-client**\n\n    In `yarn-client` mode, the driver runs in the client process, and the\n    application master is only used for requesting resources from YARN.\n\n  * **YARN-cluster**\n\n    In `yarn-cluster` mode, the Spark driver runs inside an application master\n    process which is managed by YARN on the cluster, and the client can go away\n    after initiating the application.\n\n\n## Verify the deployment\n\n### Status and Smoke Test\n\nThe services provide extended status reporting to indicate when they are ready:\n\n    juju status --format=tabular\n\nThis is particularly useful when combined with `watch` to track the on-going\nprogress of the deployment:\n\n    watch -n 0.5 juju status --format=tabular\n\nThe message for each unit will provide information about that unit's state.\nOnce they all indicate that they are ready, you can perform a \"smoke test\"\nto verify that Spark is working as expected using the built-in `smoke-test`\naction:\n\n    juju run-action spark/0 smoke-test\n\n_**Note**: The above assumes Juju 2.0 or greater. If using an earlier version\nof Juju, the syntax is `juju action do spark/0 smoke-test`._\n\n\nAfter a minute or so, you can check the results of the smoke test:\n\n    juju show-action-status\n\n_**Note**: The above assumes Juju 2.0 or greater. If using an earlier version\nof Juju, the syntax is `juju action status`._\n\nYou will see `status: completed` if the smoke test was successful, or\n`status: failed` if it was not.  You can get more information on why it failed\nvia:\n\n    juju show-action-output <action-id>\n\n_**Note**: The above assumes Juju 2.0 or greater. If using an earlier version\nof Juju, the syntax is `juju action fetch <action-id>`._\n\n\n### Verify Job History\n\nThe Job History server shows all active and finished spark jobs submitted.\nTo view the Job History server you need to expose spark (`juju expose spark`)\nand navigate to `http://{spark_master_unit_ip_address}:18080` of the\nunit acting as master.\n\n\n## Benchmarking\n\nThis charm provides several benchmarks, including the\n[Spark Bench](https://github.com/SparkTC/spark-bench) benchmarking\nsuite (if enabled), to gauge the performance of your environment.\n\nThe easiest way to run the benchmarks on this service is to relate it to the\n[Benchmark GUI][].  You will likely also want to relate it to the\n[Benchmark Collector][] to have machine-level information collected during the\nbenchmark, for a more complete picture of how the machine performed.\n\n[Benchmark GUI]: https://jujucharms.com/benchmark-gui/\n[Benchmark Collector]: https://jujucharms.com/benchmark-collector/\n\nHowever, each benchmark is also an action that can be called manually:\n\n    $ juju action do spark/0 pagerank\n    Action queued with id: 88de9367-45a8-4a4b-835b-7660f467a45e\n    $ juju action fetch --wait 0 88de9367-45a8-4a4b-835b-7660f467a45e\n    results:\n      meta:\n        composite:\n          direction: asc\n          units: secs\n          value: \"77.939000\"\n        raw: |\n          PageRank,2015-12-10-23:41:57,77.939000,71.888079,.922363,0,PageRank-MLlibConfig,,,,,10,12,,200000,4.0,1.3,0.15\n        start: 2015-12-10T23:41:34Z\n        stop: 2015-12-10T23:43:16Z\n      results:\n        duration:\n          direction: asc\n          units: secs\n          value: \"77.939000\"\n        throughput:\n          direction: desc\n          units: x/sec\n          value: \".922363\"\n    status: completed\n    timing:\n      completed: 2015-12-10 23:43:59 +0000 UTC\n      enqueued: 2015-12-10 23:42:10 +0000 UTC\n      started: 2015-12-10 23:42:15 +0000 UTC\n\nValid action names at this time are:\n\n  * logisticregression\n  * matrixfactorization\n  * pagerank\n  * sql\n  * streaming\n  * svdplusplus\n  * svm\n  * trianglecount\n  * sparkpi\n\n\n## Contact Information\n\n- <bigdata@lists.ubuntu.com>\n\n\n## Help\n\n- [Juju mailing list](https://lists.ubuntu.com/mailman/listinfo/juju)\n- [Juju community](https://jujucharms.com/community)\n",
  "readme": "<!--\n  Licensed to the Apache Software Foundation (ASF) under one or more\n  contributor license agreements.  See the NOTICE file distributed with\n  this work for additional information regarding copyright ownership.\n  The ASF licenses this file to You under the Apache License, Version 2.0\n  (the \"License\"); you may not use this file except in compliance with\n  the License.  You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n-->\n## Overview\n\nApache Spark™ is a fast and general purpose engine for large-scale data\nprocessing. Key features:\n\n * **Speed**\n\n Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster\n on disk. Spark has an advanced DAG execution engine that supports cyclic data\n flow and in-memory computing.\n\n * **Ease of Use**\n\n Write applications quickly in Java, Scala or Python. Spark offers over 80\n high-level operators that make it easy to build parallel apps, and you can use\n it interactively from the Scala and Python shells.\n\n * **General Purpose Engine**\n\n Combine SQL, streaming, and complex analytics. Spark powers a stack of\n high-level tools including Shark for SQL, MLlib for machine learning, GraphX,\n and Spark Streaming. You can combine these frameworks seamlessly in the same\n application.\n\n\n## Deployment\n\nThis charm deploys the Spark component of the Apache Bigtop platform and\nsupports running Spark in a variety of modes:\n\n * **Standalone**\n\n In this mode Spark units form a cluster that you can scale to match your needs.\n Starting with a single node:\n\n    juju deploy spark\n    juju deploy openjdk\n    juju add-relation spark openjdk\n\n You can scale the cluster by adding more spark units:\n\n    juju add-unit spark\n\n When in standalone mode, Juju ensures a single Spark master is appointed.\n The status of the unit acting as master reads \"ready (standalone - master)\",\n while the rest of the units display a status of \"ready (standalone)\".\n If you remove the master, Juju will appoint a new one. However, if a master\n fails in standalone mode, running jobs and job history will be lost.\n\n * **Standalone HA**\n\n To enable High Availability for a Spark cluster, you need to add Zookeeper to\n the deployment. To ensure a Zookeeper quorum, it is recommended that you\n deploy 3 units of the zookeeper application. For instance:\n\n    juju deploy apache-zookeeper zookeeper -n 3\n    juju add-relation spark zookeeper\n\n In this mode, you can again scale your cluster to match your needs by\n adding/removing units. Spark units report \"ready (standalone HA)\" in their\n status. If you need to identify the node acting as master, query Zookeeper\n as follows:\n\n    juju run --unit zookeeper/0 'echo \"get /spark/master_status\" | /usr/lib/zookeeper/bin/zkCli.sh'\n\n * **Yarn-client and Yarn-cluster**\n\n This charm leverages our pluggable Hadoop model with the `hadoop-plugin`\n interface. This means that you can relate this charm to a base Apache Hadoop cluster\n to run Spark jobs there. The suggested deployment method is to use the\n [hadoop-processing](https://jujucharms.com/hadoop-processing/)\n bundle and add a relation between spark and the plugin:\n\n    juju deploy hadoop-processing\n    juju add-relation plugin spark\n\n\nNote: To switch to a different execution mode, set the\n`spark_execution_mode` config variable:\n\n    juju set spark spark_execution_mode=<new_mode>\n\nSee the **Configuration** section below for supported mode options.\n\n\n## Usage\n\nOnce deployment is complete, you can manually load and run Spark batch or\nstreaming jobs in a variety of ways:\n\n  * **Spark shell**\n\nSpark’s shell provides a simple way to learn the API, as well as a powerful\ntool to analyse data interactively. It is available in either Scala or Python\nand can be run from the Spark unit as follows:\n\n       juju ssh spark/0\n       spark-shell # for interaction using scala\n       pyspark     # for interaction using python\n\n  * **Command line**\n\nSSH to the Spark unit and manually run a spark-submit job, for example:\n\n       juju ssh spark/0\n       spark-submit --class org.apache.spark.examples.SparkPi \\\n        --master yarn-client /usr/lib/spark/lib/spark-examples*.jar 10\n\n  * **Apache Zeppelin visual service**\n\nDeploy Apache Zeppelin and relate it to the Spark unit:\n\n    juju deploy apache-zeppelin zeppelin\n    juju add-relation spark zeppelin\n\nOnce the relation has been made, access the web interface at\n`http://{spark_unit_ip_address}:9090`\n\n  * **IPyNotebook for Spark**\n\nThe IPython Notebook is an interactive computational environment, in which you\ncan combine code execution, rich text, mathematics, plots and rich media.\nDeploy IPython Notebook for Spark and relate it to the Spark unit:\n\n    juju deploy apache-spark-notebook notebook\n    juju add-relation spark notebook\n\nOnce the relation has been made, access the web interface at\n`http://{spark_unit_ip_address}:8880`\n\n\n## Configuration\n\n### `spark_bench_enabled`\n\nInstall the SparkBench benchmarking suite. If `true` (the default), this charm\nwill download spark bench from the URL specified by `spark_bench_ppc64le`\nor `spark_bench_x86_64`, depending on the unit's architecture.\n\n### `spark_execution_mode`\n\nSpark has four modes of execution: local, standalone, yarn-client, and\nyarn-cluster. The default mode is `yarn-client` and can be changed by setting\nthe `spark_execution_mode` config variable.\n\n  * **Local**\n\n    In Local mode, Spark processes jobs locally without any cluster resources.\n    There are 3 ways to specify 'local' mode:\n\n    * `local`\n\n      Run Spark locally with one worker thread (i.e. no parallelism at all).\n\n    * `local[K]`\n\n      Run Spark locally with K worker threads (ideally, set this to the number\n      of cores on your machine).\n\n    * `local[*]`\n\n      Run Spark locally with as many worker threads as logical cores on your\n      machine.\n\n  * **Standalone**\n\n    In `standalone` mode, Spark launches a Master and Worker daemon on the Spark\n    unit. This mode is useful for simulating a distributed cluster environment\n    without actually setting up a cluster.\n\n  * **YARN-client**\n\n    In `yarn-client` mode, the driver runs in the client process, and the\n    application master is only used for requesting resources from YARN.\n\n  * **YARN-cluster**\n\n    In `yarn-cluster` mode, the Spark driver runs inside an application master\n    process which is managed by YARN on the cluster, and the client can go away\n    after initiating the application.\n\n\n## Verify the deployment\n\n### Status and Smoke Test\n\nThe services provide extended status reporting to indicate when they are ready:\n\n    juju status --format=tabular\n\nThis is particularly useful when combined with `watch` to track the on-going\nprogress of the deployment:\n\n    watch -n 0.5 juju status --format=tabular\n\nThe message for each unit will provide information about that unit's state.\nOnce they all indicate that they are ready, you can perform a \"smoke test\"\nto verify that Spark is working as expected using the built-in `smoke-test`\naction:\n\n    juju run-action spark/0 smoke-test\n\n_**Note**: The above assumes Juju 2.0 or greater. If using an earlier version\nof Juju, the syntax is `juju action do spark/0 smoke-test`._\n\n\nAfter a minute or so, you can check the results of the smoke test:\n\n    juju show-action-status\n\n_**Note**: The above assumes Juju 2.0 or greater. If using an earlier version\nof Juju, the syntax is `juju action status`._\n\nYou will see `status: completed` if the smoke test was successful, or\n`status: failed` if it was not.  You can get more information on why it failed\nvia:\n\n    juju show-action-output <action-id>\n\n_**Note**: The above assumes Juju 2.0 or greater. If using an earlier version\nof Juju, the syntax is `juju action fetch <action-id>`._\n\n\n### Verify Job History\n\nThe Job History server shows all active and finished spark jobs submitted.\nTo view the Job History server you need to expose spark (`juju expose spark`)\nand navigate to `http://{spark_master_unit_ip_address}:18080` of the\nunit acting as master.\n\n\n## Benchmarking\n\nThis charm provides several benchmarks, including the\n[Spark Bench](https://github.com/SparkTC/spark-bench) benchmarking\nsuite (if enabled), to gauge the performance of your environment.\n\nThe easiest way to run the benchmarks on this service is to relate it to the\n[Benchmark GUI][].  You will likely also want to relate it to the\n[Benchmark Collector][] to have machine-level information collected during the\nbenchmark, for a more complete picture of how the machine performed.\n\n[Benchmark GUI]: https://jujucharms.com/benchmark-gui/\n[Benchmark Collector]: https://jujucharms.com/benchmark-collector/\n\nHowever, each benchmark is also an action that can be called manually:\n\n    $ juju action do spark/0 pagerank\n    Action queued with id: 88de9367-45a8-4a4b-835b-7660f467a45e\n    $ juju action fetch --wait 0 88de9367-45a8-4a4b-835b-7660f467a45e\n    results:\n      meta:\n        composite:\n          direction: asc\n          units: secs\n          value: \"77.939000\"\n        raw: |\n          PageRank,2015-12-10-23:41:57,77.939000,71.888079,.922363,0,PageRank-MLlibConfig,,,,,10,12,,200000,4.0,1.3,0.15\n        start: 2015-12-10T23:41:34Z\n        stop: 2015-12-10T23:43:16Z\n      results:\n        duration:\n          direction: asc\n          units: secs\n          value: \"77.939000\"\n        throughput:\n          direction: desc\n          units: x/sec\n          value: \".922363\"\n    status: completed\n    timing:\n      completed: 2015-12-10 23:43:59 +0000 UTC\n      enqueued: 2015-12-10 23:42:10 +0000 UTC\n      started: 2015-12-10 23:42:15 +0000 UTC\n\nValid action names at this time are:\n\n  * logisticregression\n  * matrixfactorization\n  * pagerank\n  * sql\n  * streaming\n  * svdplusplus\n  * svm\n  * trianglecount\n  * sparkpi\n\n\n## Contact Information\n\n- <bigdata@lists.ubuntu.com>\n\n\n## Help\n\n- [Juju mailing list](https://lists.ubuntu.com/mailman/listinfo/juju)\n- [Juju community](https://jujucharms.com/community)\n",
  "readme_name": "README.md",
  "gatherbase_origin": "juju-charmstore"
}