{
  "name": "cinder Juju charm",
  "juju_charm_name": "cinder",
  "revision": "cs:raring/cinder-18",
  "latest": true,
  "uris": [
    "https://jujucharms.com/cinder",
    "https://jujucharms.com/cinder/raring",
    "https://jujucharms.com/cinder/raring/18",
    "https://api.jujucharms.com/v5/cinder",
    "https://api.jujucharms.com/v5/raring/cinder",
    "https://api.jujucharms.com/v5/raring/cinder-18"
  ],
  "labels": [
    "Juju charm",
    "miscellaneous",
    "Mode/Executable/Bundle/Juju Charm",
    "Type/Devopsware/Deployment/Juju",
    "Type/Devopsware/Cloud Platform/OpenStack",
    "Type/Middleware/Data Store"
  ],
  "info_url": "https://jujucharms.com/cinder",
  "package_url": "https://api.jujucharms.com/v5/raring/cinder-18/archive",
  "created": "2015-06-17T09:32:24.367Z",
  "updated": "2015-06-17T09:32:24.367Z",
  "description": "Cinder OpenStack starage service\n\nCinder is a storage service for the Openstack project\n",
  "maintainer": {
    "name": "charmers"
  },
  "juju_charm_subordinate": false,
  "juju_charm_series": "raring",
  "juju_charm_owner": "charmers",
  "requires": [
    {
      "kind": "host",
      "label": "Infrastructure/Operating System/Linux/Ubuntu",
      "version": "= raring"
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/rabbitmq",
      "self_resolve": true,
      "juju_interface": "rabbitmq",
      "juju_name": "amqp",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/ceph-client",
      "self_resolve": true,
      "juju_interface": "ceph-client",
      "juju_name": "ceph",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/hacluster",
      "self_resolve": true,
      "juju_interface": "hacluster",
      "juju_name": "ha",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/keystone",
      "self_resolve": true,
      "juju_interface": "keystone",
      "juju_name": "identity-service",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/glance",
      "self_resolve": true,
      "juju_interface": "glance",
      "juju_name": "image-service",
      "juju_role": "requirer",
      "juju_limit": 1
    },
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/requires/mysql-shared",
      "self_resolve": true,
      "juju_interface": "mysql-shared",
      "juju_name": "shared-db",
      "juju_role": "requirer",
      "juju_limit": 1
    }
  ],
  "parameters": {
    "api-listening-port": {
      "type": "int",
      "description": "OpenStack Volume API listening port.",
      "default": 8776,
      "mapping": "charm_option"
    },
    "block-device": {
      "type": "string",
      "description": "The *available* block device on which to create LVM volume group.\nMay also be set to None for deployments that will not need local\nstorage (eg, Ceph/RBD-backed volumes).\n",
      "default": "sdb",
      "mapping": "charm_option"
    },
    "ceph-osd-replication-count": {
      "type": "int",
      "description": "This value dictates the number of replicas ceph must make of any\nobject it stores withing the cinder rbd pool. Of course, this only\napplies if using Ceph as a backend store. Note that once the cinder\nrbd pool has been created, changing this value will not have any\neffect (although it can be changed in ceph by manually configuring\nyour ceph cluster).\n",
      "default": 2,
      "mapping": "charm_option"
    },
    "config-flags": {
      "type": "string",
      "description": "Comma separated list of key=value config flags to be set in cinder.conf.",
      "default": null,
      "mapping": "charm_option"
    },
    "database": {
      "type": "string",
      "description": "Database to request access.",
      "default": "cinder",
      "mapping": "charm_option"
    },
    "database-user": {
      "type": "string",
      "description": "Username to request database access.",
      "default": "cinder",
      "mapping": "charm_option"
    },
    "enabled-services": {
      "type": "string",
      "description": "If splitting cinder services between units, define which services\nto install and configure.\n",
      "default": "all",
      "mapping": "charm_option"
    },
    "glance-api-version": {
      "type": "int",
      "description": "Newer storage drivers may require the v2 Glance API to perform certain\nactions e.g. the RBD driver requires requires this to support COW\ncloning of images. This option will default to v1 for backwards\ncompatibility older glance services.\n",
      "default": 1,
      "mapping": "charm_option"
    },
    "ha-bindiface": {
      "type": "string",
      "description": "Default network interface on which HA cluster will bind to communication\nwith the other members of the HA Cluster.\n",
      "default": "eth0",
      "mapping": "charm_option"
    },
    "ha-mcastport": {
      "type": "int",
      "description": "Default multicast port number that will be used to communicate between\nHA Cluster nodes.\n",
      "default": 5401,
      "mapping": "charm_option"
    },
    "openstack-origin": {
      "type": "string",
      "description": "Repository from which to install.  May be one of the following:\ndistro (default), ppa:somecustom/ppa, a deb url sources entry,\nor a supported Cloud Archive release pocket.\n\nSupported Cloud Archive sources include: cloud:precise-folsom,\ncloud:precise-folsom/updates, cloud:precise-folsom/staging,\ncloud:precise-folsom/proposed.\n\nWhen deploying to Precise, the default distro option will use\nthe cloud:precise-folsom/updates repository instead, since Cinder\nwas not available in the Ubuntu archive for Precise and is only\navailable via the Ubuntu Cloud Archive.\n",
      "default": "distro",
      "mapping": "charm_option"
    },
    "overwrite": {
      "type": "string",
      "description": "If true, charm will attempt to overwrite block devices containin\nprevious filesystems or LVM, assuming it is not in use.\n",
      "default": "false",
      "mapping": "charm_option"
    },
    "rabbit-user": {
      "type": "string",
      "description": "Username to request access on rabbitmq-server.",
      "default": "cinder",
      "mapping": "charm_option"
    },
    "rabbit-vhost": {
      "type": "string",
      "description": "RabbitMQ virtual host to request access on rabbitmq-server.",
      "default": "openstack",
      "mapping": "charm_option"
    },
    "region": {
      "type": "string",
      "description": "OpenStack Region",
      "default": "RegionOne",
      "mapping": "charm_option"
    },
    "ssl_cert": {
      "type": "string",
      "description": "SSL certificate to install and use for API ports.  Setting this value\nand ssl_key will enable reverse proxying, point Glance's entry in the\nKeystone catalog to use https, and override any certficiate and key\nissued by Keystone (if it is configured to do so).\n",
      "default": null,
      "mapping": "charm_option"
    },
    "ssl_key": {
      "type": "string",
      "description": "SSL key to use with certificate specified as ssl_cert.",
      "default": null,
      "mapping": "charm_option"
    },
    "use-syslog": {
      "type": "boolean",
      "description": "By default, all services will log into their corresponding log files.\nSetting this to True will force all services to log to the syslog.\n",
      "default": false,
      "mapping": "charm_option"
    },
    "vip": {
      "type": "string",
      "description": "Virtual IP to use to front cinder API in ha configuration",
      "default": null,
      "mapping": "charm_option"
    },
    "vip_cidr": {
      "type": "int",
      "description": "Netmask that will be used for the Virtual IP",
      "default": 24,
      "mapping": "charm_option"
    },
    "vip_iface": {
      "type": "string",
      "description": "Network Interface where to place the Virtual IP",
      "default": "eth0",
      "mapping": "charm_option"
    },
    "volume-group": {
      "type": "string",
      "description": "Name of volume group to create and store Cinder volumes.",
      "default": "cinder-volumes",
      "mapping": "charm_option"
    }
  },
  "provides": [
    {
      "kind": "peer",
      "uri": "https://jujucharms.com/provides/cinder",
      "juju_interface": "cinder",
      "juju_name": "cinder-volume-service",
      "juju_role": "provider",
      "juju_limit": 0
    }
  ],
  "juju_peers": {
    "cluster": {
      "Name": "cluster",
      "Role": "peer",
      "Interface": "cinder-ha",
      "Optional": false,
      "Limit": 1,
      "Scope": "global"
    }
  },
  "license": "Overview\n--------\n\nThis charm provides the Cinder volume service for OpenStack.  It is intended to\nbe used alongside the other OpenStack components, starting with the Folsom\nrelease.\n\nCinder is made up of 3 separate services: an API service, a scheduler and a\nvolume service.  This charm allows them to be deployed in different\ncombination, depending on user preference and requirements.\n\nThis charm was developed to support deploying Folsom on both\nUbuntu Quantal and Ubuntu Precise.  Since Cinder is only available for\nUbuntu 12.04 via the Ubuntu Cloud Archive, deploying this charm to a\nPrecise machine will by default install Cinder and its dependencies from\nthe Cloud Archive.\n\nUsage\n-----\n\nCinder may be deployed in a number of ways.  This charm focuses on 3 main\nconfigurations.  All require the existence of the other core OpenStack\nservices deployed via Juju charms, specifically: mysql, rabbitmq-server,\nkeystone and nova-cloud-controller.  The following assumes these services\nhave already been deployed.\n\nBasic, all-in-one using local storage and iSCSI\n===============================================\n\nThe api server, scheduler and volume service are all deployed into the same\nunit.  Local storage will be initialized as a LVM phsyical device, and a volume\ngroup initialized.  Instance volumes will be created locally as logical volumes\nand exported to instances via iSCSI.  This is ideal for small-scale deployments\nor testing:\n\n    cat >cinder.cfg <<END\n    cinder:\n        block-device: sdc\n        overwrite: true\n    END\n    juju deploy --config=cinder.cfg cinder\n    juju add-relation cinder keystone\n    juju add-relation cinder mysql\n    juju add-relation cinder rabbitmq-server\n    juju add-relation cinder nova-cloud-controller\n\nSeparate volume units for scale out, using local storage and iSCSI\n==================================================================\n\nSeparating the volume service from the API service allows the storage pool\nto easily scale without the added complexity that accompanies load-balancing\nthe API server.  When we've exhausted local storage on volume server, we can\nsimply add-unit to expand our capacity.  Future requests to allocate volumes\nwill be distributed across the pool of volume servers according to the\navailability of storage space.\n\n    cat >cinder.cfg <<END\n    cinder-api:\n        enabled-services: api, scheduler\n    cinder-volume:\n        enabled-services: volume\n        block-device: sdc\n        overwrite: true\n    END\n    juju deploy --config=cinder.cfg cinder cinder-api\n    juju deploy --config=cinder.cfg cinder cinder-volume\n    juju add-relation cinder-api mysql\n    juju add-relation cinder-api rabbitmq-server\n    juju add-relation cinder-api keystone\n    juju add-relation cinder-api nova-cloud-controller\n    juju add-relation cinder-volume mysql\n    juju add-relation cinder-volume rabbitmq-server\n\n    # When more storage is needed, simply add more volume servers.\n    juju add-unit cinder-volume\n\nAll-in-one using Ceph-backed RBD volumes\n========================================\n\nAll 3 services can be deployed to the same unit, but instead of relying\non local storage to back volumes an external Ceph cluster is used.  This\nallows scalability and redundancy needs to be satisified and Cinder's RBD\ndriver used to create, export and connect volumes to instances.  This assumes\na functioning Ceph cluster has already been deployed using the official Ceph\ncharm and a relation exists between the Ceph service and the nova-compute\nservice.\n\n    cat >cinder.cfg <<END\n    cinder:\n        block-device: None\n    END\n    juju deploy --config=cinder.cfg cinder\n    juju add-relation cinder ceph\n    juju add-relation cinder keystone\n    juju add-relation cinder mysql\n    juju add-relation cinder rabbitmq-server\n    juju add-relation cinder nova-cloud-controller\n\n\nConfiguration\n-------------\n\nThe default value for most config options should work for most deployments.\n\nUsers should be aware of three options, in particular:\n\nopenstack-origin:  Allows Cinder to be installed from a specific apt repository.\n                   See config.yaml for a list of supported sources.\n\nblock-device:  When using local storage, a block device should be specified to\n               back a LVM volume group.  It's important this device exists on\n               all nodes that the service may be deployed to.\n\noverwrite:  Whether or not to wipe local storage that of data that may prevent\n            it from being initialized as a LVM phsyical device.  This includes\n            filesystems and partition tables.  *CAUTION*\n\nenabled-services:  Can be used to separate cinder services between service\n                   service units (see previous section)\n\nContact Information\n-------------------\n\nAuthor: Adam Gandelman <adamg@canonical.com>\nReport bugs at: http://bugs.launchpad.net/charms\nLocation: http://jujucharms.com\n",
  "readme": "Overview\n--------\n\nThis charm provides the Cinder volume service for OpenStack.  It is intended to\nbe used alongside the other OpenStack components, starting with the Folsom\nrelease.\n\nCinder is made up of 3 separate services: an API service, a scheduler and a\nvolume service.  This charm allows them to be deployed in different\ncombination, depending on user preference and requirements.\n\nThis charm was developed to support deploying Folsom on both\nUbuntu Quantal and Ubuntu Precise.  Since Cinder is only available for\nUbuntu 12.04 via the Ubuntu Cloud Archive, deploying this charm to a\nPrecise machine will by default install Cinder and its dependencies from\nthe Cloud Archive.\n\nUsage\n-----\n\nCinder may be deployed in a number of ways.  This charm focuses on 3 main\nconfigurations.  All require the existence of the other core OpenStack\nservices deployed via Juju charms, specifically: mysql, rabbitmq-server,\nkeystone and nova-cloud-controller.  The following assumes these services\nhave already been deployed.\n\nBasic, all-in-one using local storage and iSCSI\n===============================================\n\nThe api server, scheduler and volume service are all deployed into the same\nunit.  Local storage will be initialized as a LVM phsyical device, and a volume\ngroup initialized.  Instance volumes will be created locally as logical volumes\nand exported to instances via iSCSI.  This is ideal for small-scale deployments\nor testing:\n\n    cat >cinder.cfg <<END\n    cinder:\n        block-device: sdc\n        overwrite: true\n    END\n    juju deploy --config=cinder.cfg cinder\n    juju add-relation cinder keystone\n    juju add-relation cinder mysql\n    juju add-relation cinder rabbitmq-server\n    juju add-relation cinder nova-cloud-controller\n\nSeparate volume units for scale out, using local storage and iSCSI\n==================================================================\n\nSeparating the volume service from the API service allows the storage pool\nto easily scale without the added complexity that accompanies load-balancing\nthe API server.  When we've exhausted local storage on volume server, we can\nsimply add-unit to expand our capacity.  Future requests to allocate volumes\nwill be distributed across the pool of volume servers according to the\navailability of storage space.\n\n    cat >cinder.cfg <<END\n    cinder-api:\n        enabled-services: api, scheduler\n    cinder-volume:\n        enabled-services: volume\n        block-device: sdc\n        overwrite: true\n    END\n    juju deploy --config=cinder.cfg cinder cinder-api\n    juju deploy --config=cinder.cfg cinder cinder-volume\n    juju add-relation cinder-api mysql\n    juju add-relation cinder-api rabbitmq-server\n    juju add-relation cinder-api keystone\n    juju add-relation cinder-api nova-cloud-controller\n    juju add-relation cinder-volume mysql\n    juju add-relation cinder-volume rabbitmq-server\n\n    # When more storage is needed, simply add more volume servers.\n    juju add-unit cinder-volume\n\nAll-in-one using Ceph-backed RBD volumes\n========================================\n\nAll 3 services can be deployed to the same unit, but instead of relying\non local storage to back volumes an external Ceph cluster is used.  This\nallows scalability and redundancy needs to be satisified and Cinder's RBD\ndriver used to create, export and connect volumes to instances.  This assumes\na functioning Ceph cluster has already been deployed using the official Ceph\ncharm and a relation exists between the Ceph service and the nova-compute\nservice.\n\n    cat >cinder.cfg <<END\n    cinder:\n        block-device: None\n    END\n    juju deploy --config=cinder.cfg cinder\n    juju add-relation cinder ceph\n    juju add-relation cinder keystone\n    juju add-relation cinder mysql\n    juju add-relation cinder rabbitmq-server\n    juju add-relation cinder nova-cloud-controller\n\n\nConfiguration\n-------------\n\nThe default value for most config options should work for most deployments.\n\nUsers should be aware of three options, in particular:\n\nopenstack-origin:  Allows Cinder to be installed from a specific apt repository.\n                   See config.yaml for a list of supported sources.\n\nblock-device:  When using local storage, a block device should be specified to\n               back a LVM volume group.  It's important this device exists on\n               all nodes that the service may be deployed to.\n\noverwrite:  Whether or not to wipe local storage that of data that may prevent\n            it from being initialized as a LVM phsyical device.  This includes\n            filesystems and partition tables.  *CAUTION*\n\nenabled-services:  Can be used to separate cinder services between service\n                   service units (see previous section)\n\nContact Information\n-------------------\n\nAuthor: Adam Gandelman <adamg@canonical.com>\nReport bugs at: http://bugs.launchpad.net/charms\nLocation: http://jujucharms.com\n",
  "readme_name": "README.md",
  "gatherbase_origin": "juju-charmstore"
}