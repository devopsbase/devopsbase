{
  "name": "spark-platform Chef cookbook",
  "chef_cookbook_name": "spark-platform",
  "revision": "1.1.0",
  "uris": [
    "https://supermarket.chef.io/cookbooks/spark-platform",
    "https://supermarket.chef.io/cookbooks/spark-platform/download",
    "https://supermarket.chef.io/cookbooks/spark-platform/versions/1.1.0",
    "https://supermarket.chef.io/cookbooks/spark-platform/versions/1.1.0/download",
    "https://supermarket.chef.io/api/v1/cookbooks/spark-platform/versions/1.1.0",
    "https://supermarket.chef.io/api/v1/cookbooks/spark-platform",
    "https://supermarket.chef.io/api/v1/cookbooks/spark-platform/versions/1.1.0/download"
  ],
  "labels": [
    "Chef cookbook",
    "Applications",
    "Executable/Script/Chef Cookbook",
    "Mode/Executable/Script/Chef Cookbook",
    "Type/Middleware/Web Server/Apache HTTP Server"
  ],
  "info_url": "https://supermarket.chef.io/cookbooks/spark-platform",
  "package_url": "https://supermarket.chef.io/api/v1/cookbooks/spark-platform/versions/1.1.0/download",
  "deprecated": false,
  "created": "2016-05-31T17:48:16.902Z",
  "updated": "2016-06-29T19:26:06.685Z",
  "description": "Install and configure Apache Spark",
  "maintainer": {
    "name": "samuel_bernard",
    "email": "dps.team@s4m.io"
  },
  "license": "Apache 2.0",
  "chef_foodcritic_failure": false,
  "chef_up_for_adoption": null,
  "rating": null,
  "followers_count": 0,
  "downloads_count": 62,
  "downloads_count_revision": 34,
  "repository_url": "https://gitlab.com/s4m-chef-repositories/spark-platform",
  "issues_url": "https://gitlab.com/s4m-chef-repositories/spark-platform/issues",
  "chef_source_url": "https://gitlab.com/s4m-chef-repositories/spark-platform",
  "latest": true,
  "gatherbase_origin": "chef-supermarket",
  "readme_name": "README.md",
  "readme": "Spark Platform\n==============\n\nDescription\n-----------\n\nApache Sparkâ„¢ is a fast and general engine for large-scale data processing.\n\nThis cookbook is designed to install and configure Spark.\n\nRequirements\n------------\n\n### Cookbooks and gems\n\nDeclared in [metadata.rb](metadata.rb) and in [Gemfile](Gemfile).\n\n### Platforms\n\nA *systemd* managed distribution:\n- RHEL Family 7, tested on Centos\n\nUsage\n-----\n\n### First Setup\n\nThe recommended way to use this cookbook is through the creation of a role\nper **spark** cluster. This enables the search by role feature, allowing a\nsimple service discovery.\n\nIn fact, there are two ways to configure the search:\n1. with a static configuration through a list of hostnames (attributes `hosts`\n   that is `['spark-platform']['hosts']`)\n2. with a real search, performed on a role (attributes `role` and `size`\n   like in `['spark-platform']['role']`). The role should be in the run-list\n   of all nodes of the cluster. The size is a safety and should be the number\n   of nodes in the cluster.\n\nIf hosts is configured, `role` and `size` are ignored\n\nSee [roles](test/integration/roles) for some examples and\n[Cluster Search][cluster-search] documentation for more information.\n\n### Master High Availability\n\nTo install properly a HA **spark** cluster, set\n`['spark-platform']['master_ha_with_zk']` to true. Then set the number of\nmasters you want with `['spark-platform']['n_of_masters']`.\n\nYou also need a **Zookeeper** cluster. This is not in the scope of this\ncookbook but if you need one, you should consider using [Zookeeper\nPlatform][zookeeper-platform].\n\nThe configuration of Zookeeper hosts use search and is done similarly as for\n**spark** hosts, _ie_ with a static list of hostnames or by using a search on\na role. The attribute to configure is `['spark-platform']['zookeeper']`.\n\n### Java\n\nBy default, this cookbook installs *openjdk* from the official repositories\n*(openjdk-headless 8 on centos 7)* just before starting the service. You can\nchange this behavior by setting `node['spark-platform']['java']` to `\"\"`, or\nchoose your package by setting the package name in\n`node['spark-platform']['java'][node[:platform]]`.\n\n### Test\n\nThis cookbook is fully tested through the installation of the full platform\nin docker hosts. This uses kitchen, docker and some monkey-patching.\n\nIf you run `kitchen list`, you will see 4 suites:\n\n- zookeeper-centos-7\n- spark-platform-1-centos-7\n- spark-platform-2-centos-7\n- spark-platform-3-centos-7\n\nEach corresponds to a different node in the cluster. They are connected through\na bridge network named *kitchen*, which is created if necessary.\n\nFor more information, see [.kitchen.yml](.kitchen.yml) and [test](test)\ndirectory.\n\n### Local cluster\n\nThe cluster installed with `kitchen converge` is fully working and can thus be\nused as a local cluster for developments tests.\n\nYou can access it by using internal DNS of the docker network named *kitchen*\nor by declaring each node in your hosts file. You can get each IP by\nrunning:\n\n    docker inspect --format \\\n      '{{.NetworkSettings.Networks.kitchen.IPAddress}}' container_name\n\n### Spark group\n\nIn standalone mode, it is not possible to execute spark jobs with different\nusers. This means that the result files will always be owned by spark user. To\nbe able to use different user accounts, a solution is to add all users to spark\ngroup and make the files group modifiable. In this cookbook, the spark group is\ncreated with \"append true\" option so other cookbooks can modify it to add\nusers.\n\nAttributes\n----------\n\nConfiguration is done by overriding default attributes. All configuration keys\nhave a default defined in [attributes/default.rb](attributes/default.rb).\nPlease read it to have a comprehensive view of what and how you can configure\nthis cookbook behavior.\n\nRecipes\n-------\n\n### default\n\nInclude other recipes, that is:\n\n1. user\n2. install\n3. config\n4. systemd\n5. service\n\n### user\n\nCreate user and group for Apache Spark.\n\n### install\n\nInstall Apache Spark using a tarball package.\n\n### config\n\nConfigure Apache Spark, searching for other cluster members if available.\n\n### systemd\n\nCreate systemd unit files\n\n### service\n\nEnable and start services\n\nResources/Providers\n-------------------\n\nNone.\n\nChangelog\n---------\n\nAvailable in [CHANGELOG.md](CHANGELOG.md).\n\nContributing\n------------\n\nPlease read carefully [CONTRIBUTING.md](CONTRIBUTING.md) before making a merge\nrequest.\n\nLicense and Author\n------------------\n\n- Author:: Samuel Bernard (<samuel.bernard@s4m.io>)\n- Author:: Florian Philippon (<florian.philippon@s4m.io>)\n\n```text\nCopyright (c) 2016 Sam4Mobile\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n",
  "requires": [
    {
      "kind": "host",
      "label": "centos",
      "revision": ">= 7.1",
      "one_of_group": "os"
    },
    {
      "kind": "env",
      "uri": "https://supermarket.chef.io/cookbooks/ark",
      "revision": ">= 0.0.0",
      "self_resolve": true
    },
    {
      "kind": "env",
      "uri": "https://supermarket.chef.io/cookbooks/cluster-search",
      "revision": ">= 0.0.0",
      "self_resolve": true
    }
  ]
}