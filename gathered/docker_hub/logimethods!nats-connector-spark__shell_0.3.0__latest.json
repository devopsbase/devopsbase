{
  "dockerhub": {
    "web_url": "https://hub.docker.com/r/logimethods/nats-connector-spark",
    "repository_url": "https://hub.docker.com/v2/repositories/logimethods/nats-connector-spark",
    "tags_url": "https://hub.docker.com/v2/repositories/logimethods/nats-connector-spark/tags",
    "dockerfile_url": "https://hub.docker.com/v2/repositories/logimethods/nats-connector-spark/dockerfile",
    "autobuild_url": "https://hub.docker.com/v2/repositories/logimethods/nats-connector-spark/autobuild",
    "user": "logimethods",
    "name": "nats-connector-spark",
    "namespace": "logimethods",
    "status": 1,
    "is_private": false,
    "is_automated": true,
    "star_count": 0,
    "pull_count": 592,
    "last_updated": "2016-10-05T14:38:53.723036Z",
    "permissions": {
      "read": true,
      "write": false,
      "admin": false
    },
    "tags": [
      {
        "name": "shell_0.3.0",
        "full_size": 527583718,
        "id": 5251143,
        "repository": 795423,
        "creator": 704331,
        "last_updater": 704331,
        "last_updated": "2016-10-05T14:38:52.142604Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "app_0.3.0",
        "full_size": 176513146,
        "id": 5251130,
        "repository": 795423,
        "creator": 704331,
        "last_updater": 704331,
        "last_updated": "2016-10-05T14:37:08.046156Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "monitor_0.3.0",
        "full_size": 100102725,
        "id": 5251115,
        "repository": 795423,
        "creator": 704331,
        "last_updater": 704331,
        "last_updated": "2016-10-05T14:36:41.935341Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "inject_0.3.0",
        "full_size": 125066239,
        "id": 5251196,
        "repository": 795423,
        "creator": 704331,
        "last_updater": 704331,
        "last_updated": "2016-10-05T14:36:41.227777Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "shell_0.2.0",
        "full_size": 527583727,
        "id": 4343371,
        "repository": 795423,
        "creator": 704331,
        "last_updater": 704331,
        "last_updated": "2016-10-04T15:26:25.807823Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "monitor_0.2.0",
        "full_size": 100102699,
        "id": 4343360,
        "repository": 795423,
        "creator": 704331,
        "last_updater": 704331,
        "last_updated": "2016-10-04T15:25:19.966665Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "inject_0.2.0",
        "full_size": 125066219,
        "id": 4343147,
        "repository": 795423,
        "creator": 704331,
        "last_updater": 704331,
        "last_updated": "2016-10-04T15:25:18.709168Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "app_0.2.0",
        "full_size": 176513088,
        "id": 4343342,
        "repository": 795423,
        "creator": 704331,
        "last_updater": 704331,
        "last_updated": "2016-10-04T15:22:21.672986Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "inject",
        "full_size": 185262413,
        "id": 3820974,
        "repository": 795423,
        "creator": 704331,
        "last_updater": 704331,
        "last_updated": "2016-08-16T14:58:42.490937Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "shell",
        "full_size": 527583719,
        "id": 3807887,
        "repository": 795423,
        "creator": 704331,
        "last_updater": 704331,
        "last_updated": "2016-08-16T14:15:43.075632Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      }
    ],
    "build_name": "Logimethods/docker-nats-connector-spark"
  },
  "name": "logimethods/nats-connector-spark Docker container",
  "description": "Project to demonstrate the use of the NATS to Spark & NATS to Gatling Connectors.",
  "readme": "# docker-nats-connector-spark\nA collection of Docker Images to illustrate the use of the [nats-connector-spark](https://github.com/Logimethods/nats-connector-spark), [nats-connector-spark-scala](https://github.com/Logimethods/nats-connector-spark-scala) & [nats-connector-gatling](https://github.com/Logimethods/nats-connector-gatling) librairies.\n\n[![MIT License](https://img.shields.io/npm/l/express.svg)](http://opensource.org/licenses/MIT)\n[![Issues](https://img.shields.io/github/issues/Logimethods/docker-nats-connector-spark.svg)](https://github.com/Logimethods/docker-nats-connector-spark/issues)\n[![wercker status](https://app.wercker.com/status/ae3458297bed4e5921beaba70acf173b/s/master \"wercker status\")](https://app.wercker.com/project/bykey/ae3458297bed4e5921beaba70acf173b)\n\n[![Dockerhub](http://dockeri.co/image/logimethods/nats-connector-spark)](https://hub.docker.com/r/logimethods/nats-connector-spark/)\n\n## Usage\n```Shell\ncd compose\ndocker-compose up\n```\nThen, to stop this application (from another terminal):\n```Shell\ncd compose\ndocker-compose down\n```\n\n## What it does...\n1) Docker-compose will instantiate and run a collection of Docker Containers defined by [docker-compose.yml](https://github.com/Logimethods/docker-nats-connector-spark/blob/master/compose/docker-compose.yml)\n\n2) Gatling does emit every 15 sec a stream of values from 100 to 150 into NATS. See [NatsInjection.scala](https://github.com/Logimethods/docker-nats-connector-spark/blob/master/inject/user-files/simulations/nats/NatsInjection.scala).\n\n3) Those values are transmitted from NATS to Spark Streaming (running on a Spark cluster), then the maximum value of each stream is computed by Spark & reemited into NATS. See [SparkProcessor.scala](https://github.com/Logimethods/docker-nats-connector-spark/blob/master/app/src/main/scala/com/logimethods/nats/connector/spark/app/SparkProcessor.scala)\n\n4) Finally, those values are monitored and printed into the console. See [NatsOutputMonitor.scala](https://github.com/Logimethods/docker-nats-connector-spark/blob/master/monitor/src/main/scala/com/logimethods/nats/connector/spark/monitor/NatsOutputMonitor.scala)\n\n![NATS-Docker_Containers.png](NATS-Docker_Containers.png \"Data flow\")\n\n## Build\n\nThose Docker Images are pushed to [dockerhub:logimethods/nats-connector-spark](https://hub.docker.com/r/logimethods/nats-connector-spark/) and build there.\n\n![build.png](build.png \"Global Build\")\n\n## Release Notes\n### Version 0.1.0\n* Spark is based on version 1.5.2 so to be able to use docker-compose without hostname constrains.\n\n### Version 0.2.0\n* To be able to use Spark version 1.6.2, docker-compose containers need to belong to an external network (which enforce a hostname without underscore). See [Switch to using hyphens as a separator in hostnames](https://github.com/docker/compose/issues/229):\n```Shell\n$ docker network create spark\n```\n`docker-compose.yml`:\n```YAML\n...\nnetworks:\n  default:\n    external:\n      name: spark\n```\n\n## Links\n* [nats-connector-gatling on Github](https://github.com/Logimethods/nats-connector-gatling)\n* [nats-connector-spark on Github](https://github.com/Logimethods/nats-connector-spark)\n* [nats-connector-spark-scala on Github](https://github.com/Logimethods/nats-connector-spark-scala)\n* [docker-nats-connector-spark on Github](https://github.com/Logimethods/docker-nats-connector-spark)\n* [nats-connector-gatling build on Wercker](https://app.wercker.com/logimethods/nats-connector-gatling)\n* [logimethods/maven-nats on Github](https://hub.docker.com/r/logimethods/maven-nats/builds/)\n* [nats-connector-spark build on Wercker](https://app.wercker.com/logimethods/nats-connector-spark)\n* [logimethods/sbt-nats on Github](https://hub.docker.com/r/logimethods/sbt-nats/builds/)\n* [nats-connector-spark-scala build on Wercker](https://app.wercker.com/logimethods/nats-connector-spark-scala/)\n* [logimethods on Nexus](https://oss.sonatype.org/#nexus-search;quick~logimethods)\n* [docker-nats-connector-spark build on Wercker](https://app.wercker.com/logimethods/docker-nats-connector-spark)\n* [nats-connector-spark on Docker Hub](https://hub.docker.com/r/logimethods/nats-connector-spark/)\n\n## Outcome (digest)\n```Shell\ncompose$ docker-compose up\nspark-master is up-to-date\nspark-slave2 is up-to-date\nnats-main is up-to-date\nspark-slave1 is up-to-date\nRecreating compose_monitor_1\nRecreating main-app\nRecreating compose_gatling_1\nAttaching to spark-master, spark-slave2, nats-main, spark-slave1, main-app, compose_gatling_1, compose_monitor_1\nmain-app        | Will process messages from INPUT to OUTPUT\ngatling_1       | GATLING_HOME is set to /opt/gatling\nmain-app        | SPARK_MASTER_URL = spark://spark-master:7077\nspark-master    | 16/07/11 18:13:55 INFO Master: Registered signal handlers for [TERM, HUP, INT]\nmain-app        | 16/07/13 14:03:10 INFO SparkContext: Running Spark version 1.5.2\nspark-slave2    | 16/07/11 18:13:54 INFO Worker: Registered signal handlers for [TERM, HUP, INT]\nnats-main       | [1] 2016/07/11 18:13:53.495906 [INF] Starting nats-server version 0.8.1\nspark-slave2    | 16/07/11 18:13:56 INFO Remoting: Starting remoting\nspark-slave2    | 16/07/11 18:13:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkWorker@172.20.0.4:37988]\nspark-slave2    | 16/07/11 18:13:57 INFO Utils: Successfully started service 'sparkWorker' on port 37988.\nspark-master    | 16/07/11 18:13:58 INFO Remoting: Starting remoting\nspark-master    | 16/07/11 18:13:58 INFO Utils: Successfully started service 'sparkMaster' on port 7077.\nspark-master    | 16/07/11 18:13:58 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkMaster@spark-master:7077]\nspark-master    | 16/07/11 18:13:58 INFO Master: Starting Spark master at spark://spark-master:7077\nspark-master    | 16/07/11 18:13:58 INFO Master: Running Spark version 1.5.2\nspark-master    | 16/07/11 18:13:59 INFO Utils: Successfully started service 'MasterUI' on port 8080.\nspark-master    | 16/07/11 18:13:59 INFO MasterWebUI: Started MasterWebUI at http://172.20.0.3:8080\nspark-master    | 16/07/11 18:13:59 INFO Utils: Successfully started service on port 6066.\nspark-master    | 16/07/11 18:13:59 INFO StandaloneRestServer: Started REST server for submitting applications on port 6066\nspark-master    | 16/07/11 18:14:05 INFO Master: Registering app NATS Data Processing\nspark-master    | 16/07/11 18:14:05 INFO Master: Registered app NATS Data Processing with ID app-20160711181405-0000\nnats-main       | [1] 2016/07/11 18:13:53.495988 [INF] Starting http monitor on :8222\nspark-slave2    | 16/07/11 18:13:57 INFO Worker: Starting Spark worker 172.20.0.4:37988 with 4 cores, 8.7 GB RAM\nmain-app        | 16/07/13 14:03:11 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.20.0.8:37838]\nmain-app        | 16/07/13 14:03:11 INFO Utils: Successfully started service 'sparkDriver' on port 37838.\nmain-app        | 16/07/13 14:03:11 INFO AppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\nmain-app        | 16/07/13 14:03:12 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20160713140311-0001\nnats-main       | [1] 2016/07/11 18:13:53.496144 [INF] Listening for route connections on 0.0.0.0:6222\nnats-main       | [1] 2016/07/11 18:13:53.496196 [INF] Listening for client connections on 0.0.0.0:4222\nnats-main       | [1] 2016/07/11 18:13:53.496236 [INF] Server is ready\nnats-main       | [1] 2016/07/11 18:23:47.824141 [INF] Starting nats-server version 0.8.1\nspark-master    | 16/07/11 18:23:51 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkMaster@spark-master:7077]\nspark-master    | 16/07/11 18:23:51 INFO Utils: Successfully started service 'sparkMaster' on port 7077.\nspark-master    | 16/07/11 18:23:51 INFO Master: Starting Spark master at spark://spark-master:7077\nspark-master    | 16/07/11 18:23:52 INFO Master: I have been elected leader! New state: ALIVE\nspark-slave2    | 16/07/11 18:13:57 INFO WorkerWebUI: Started WorkerWebUI at http://172.20.0.4:8081\nspark-slave2    | 16/07/11 18:13:57 INFO Worker: Connecting to master spark-master:7077...\nmain-app        | 16/07/13 14:03:12 INFO AppClient$ClientEndpoint: Executor added: app-20160713140311-0001/0 on worker-20160713140115-172.20.0.4-41459 (172.20.0.4:41459) with 4 cores\nmain-app        | 16/07/13 14:03:12 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160713140311-0001/0 on hostPort 172.20.0.4:41459 with 4 cores, 1024.0 MB RAM\nspark-master    | 16/07/11 18:32:23 INFO Master: Registered app NATS Data Processing with ID app-20160711183223-0000\nmain-app        | 16/07/13 14:03:12 INFO SparkContext: Added JAR /app/nats-connector-spark-0.1.0.jar at http://172.20.0.8:46335/jars/nats-connector-spark-0.1.0.jar with timestamp 1468418592350\nmain-app        | NATS_URI = nats://nats-main:4222\nspark-slave1    | 16/07/11 18:14:03 INFO Worker: Connecting to master spark-master:7077...\nspark-slave1    | 16/07/11 18:14:04 INFO Worker: Successfully registered with master spark://spark-master:7077\nspark-slave2    | 16/07/11 18:14:07 INFO Worker: Successfully registered with master spark://spark-master:7077\nspark-slave2    | 16/07/11 18:14:08 INFO Worker: Asked to launch executor app-20160711181405-0000/1 for NATS Data Processing\nmonitor_1       | Will be listening to messages from OUTPUT\nmain-app        | -------------------------------------------\nmain-app        | Time: 1468418602000 ms\nmain-app        | -------------------------------------------\nmain-app        | \ngatling_1       | Simulation com.logimethods.nats.demo.NatsInjection started...\ngatling_1       | Will emit messages to INPUT\ngatling_1       | \ngatling_1       | ================================================================================\ngatling_1       | 2016-07-13 14:03:31                                           0s elapsed\ngatling_1       | ---- NATS call -----------------------------------------------------------------\ngatling_1       | [                                                                          ]  0%\ngatling_1       |           waiting: 900    / active: 0      / done:0     \ngatling_1       | ---- Requests ------------------------------------------------------------------\ngatling_1       | > Global                                                   (OK=0      KO=0     )\ngatling_1       | \ngatling_1       | ================================================================================\ngatling_1       | \nmain-app        | -------------------------------------------\nmain-app        | Time: 1468418612000 ms\nmain-app        | -------------------------------------------\nmain-app        | 140\nmain-app        | \nmonitor_1       | Received message: 140\nmain-app        | -------------------------------------------\nmain-app        | Time: 1468418614000 ms\nmain-app        | -------------------------------------------\nmain-app        | 150\nmain-app        | \nmonitor_1       | Received message: 150\ngatling_1       | \ngatling_1       | ================================================================================\ngatling_1       | 2016-07-13 14:03:36                                           5s elapsed\ngatling_1       | ---- NATS call -----------------------------------------------------------------\ngatling_1       | [######                                                                    ]  8%\ngatling_1       |           waiting: 825    / active: 0      / done:75    \ngatling_1       | ---- Requests ------------------------------------------------------------------\ngatling_1       | > Global                                                   (OK=0      KO=0     )\ngatling_1       | \ngatling_1       | ================================================================================\ngatling_1       | \nmain-app        | -------------------------------------------\nmain-app        | Time: 1468418618000 ms\nmain-app        | -------------------------------------------\nmain-app        | 150\nmain-app        | \nmonitor_1       | Received message: 150\n...\n```\n\n## License\n\n(The MIT License)\n\nCopyright (c) 2016 Logimethods.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to\ndeal in the Software without restriction, including without limitation the\nrights to use, copy, modify, merge, publish, distribute, sublicense, and/or\nsell copies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\nIN THE SOFTWARE.\n",
  "dockerfile": "FROM gettyimages/spark:1.6.1-hadoop-2.6\nADD 0/scala-library.jar 1/nats-connector-spark-0.1.0.jar 2/jnats-0.3.1.jar 3/slf4j-api-1.7.5.jar 4/slf4j-simple-1.7.5.jar ./lib_add/\nADD 5/docker-nats-connector-spark_2.10-0.1.0.jar ./lib_main/docker-nats-connector-spark_2.10-0.1.0.jar\nADD 6/script ./script\nADD 7/command ./\nENV SPARK_LIBS_PATHS=\"./lib_add/scala-library.jar:./lib_add/nats-connector-spark-0.1.0.jar:./lib_add/jnats-0.3.1.jar:./lib_add/slf4j-api-1.7.5.jar:./lib_add/slf4j-simple-1.7.5.jar:./lib_main/docker-nats-connector-spark_2.10-0.1.0.jar\"\nCMD [\"sleep\", \"infinity\"]",
  "dockerfile_json": {
    "add": [
      {
        "source": "0/scala-library.jar",
        "dest": "1/nats-connector-spark-0.1.0.jar"
      },
      {
        "source": "5/docker-nats-connector-spark_2.10-0.1.0.jar",
        "dest": "./lib_main/docker-nats-connector-spark_2.10-0.1.0.jar"
      },
      {
        "source": "6/script",
        "dest": "./script"
      },
      {
        "source": "7/command",
        "dest": "./"
      }
    ],
    "expose": [],
    "volume": [],
    "run": [],
    "workdir": [],
    "from": "gettyimages/spark:1.6.1-hadoop-2.6",
    "env": {
      "SPARK_LIBS_PATHS": "\"./lib_add/scala-library.jar:./lib_add/nats-connector-spark-0.1.0.jar:./lib_add/jnats-0.3.1.jar:./lib_add/slf4j-api-1.7.5.jar:./lib_add/slf4j-simple-1.7.5.jar:./lib_main/docker-nats-connector-spark_2.10-0.1.0.jar\""
    },
    "cmd": "[\"sleep\", \"infinity\"]"
  },
  "source_repository_url": "https://github.com/Logimethods/docker-nats-connector-spark.git",
  "source_repository_type": "git",
  "source_repository_provider": "github",
  "source_repository_web_url": "https://github.com/Logimethods/docker-nats-connector-spark",
  "docker_repository": "logimethods/nats-connector-spark",
  "docker_image": "logimethods/nats-connector-spark",
  "parameters_schema": {
    "SPARK_LIBS_PATHS": {
      "default": "\"./lib_add/scala-library.jar:./lib_add/nats-connector-spark-0.1.0.jar:./lib_add/jnats-0.3.1.jar:./lib_add/slf4j-api-1.7.5.jar:./lib_add/slf4j-simple-1.7.5.jar:./lib_main/docker-nats-connector-spark_2.10-0.1.0.jar\"",
      "type": "string",
      "mapping": "environment_variable"
    },
    "exposed_ports": {
      "default": [],
      "type": "json_array"
    }
  },
  "type": "docker",
  "docker_name": "logimethods/nats-connector-spark",
  "revision": "shell_0.3.0",
  "uris": [
    "https://hub.docker.com/r/logimethods/nats-connector-spark",
    "https://hub.docker.com/v2/repositories/logimethods/nats-connector-spark",
    "https://github.com/Logimethods/docker-nats-connector-spark.git"
  ],
  "labels": [
    "Docker",
    "Binding/Region/Europe/EU",
    "Binding/Region/North America/US",
    "Type/Devopsware/Logging",
    "Type/Infrastructure/Operating System"
  ],
  "info_url": "https://hub.docker.com/r/logimethods/nats-connector-spark",
  "requires": [
    {
      "kind": "host",
      "label": "Docker Engine"
    }
  ],
  "latest": true,
  "gatherbase_origin": "docker-hub"
}