{
  "dockerhub": {
    "web_url": "https://hub.docker.com/r/khainguyendinh/django-dynamic-scraper",
    "repository_url": "https://hub.docker.com/v2/repositories/khainguyendinh/django-dynamic-scraper",
    "tags_url": "https://hub.docker.com/v2/repositories/khainguyendinh/django-dynamic-scraper/tags",
    "dockerfile_url": "https://hub.docker.com/v2/repositories/khainguyendinh/django-dynamic-scraper/dockerfile",
    "autobuild_url": "https://hub.docker.com/v2/repositories/khainguyendinh/django-dynamic-scraper/autobuild",
    "user": "khainguyendinh",
    "name": "django-dynamic-scraper",
    "namespace": "khainguyendinh",
    "status": 1,
    "is_private": false,
    "is_automated": true,
    "star_count": 1,
    "pull_count": 77,
    "last_updated": "2016-06-24T11:47:21.034182Z",
    "permissions": {
      "read": true,
      "write": false,
      "admin": false
    },
    "tags": [
      {
        "name": "latest",
        "full_size": 254511921,
        "id": 3572914,
        "repository": 741429,
        "creator": 750741,
        "last_updater": 750741,
        "last_updated": "2016-06-24T11:47:20.681229Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "runtime",
        "full_size": 254512087,
        "id": 3412032,
        "repository": 741429,
        "creator": 750741,
        "last_updater": 750741,
        "last_updated": "2016-06-24T11:38:35.686436Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      }
    ],
    "build_name": "khainguyen95/django-dynamic-scraper"
  },
  "name": "khainguyendinh/django-dynamic-scraper Docker container",
  "description": "django-dynamic-scraper (DDS) and scrapy",
  "readme": "Django-dynamic-scraper\n===================\n\nDjango-dynamic-scraper use scrapy base on django framework and use admin django interface create scrapy crawl many website.\n\n----------\n\n## Requirements:\n\n\n\n>  - Python 2.7+ or 3.4+\n>  - Django 1.8/1.9\n>  - Scrapy 1.1\n>  - Scrapy-djangoitem 1.1\n>  - Python JSONPath RW 1.4+\n>  - Python future\n>  - scrapyd\n>  - django-celery\n>  - django-dynamic-scraper\n\n\n\n## Documents\n-------------\n\n[Tutorial DDS](https://django-dynamic-scraper.readthedocs.io)\n\n[Scrapyd-client](https://github.com/scrapy/scrapyd-client)\n\n[DjangoItem in scrapy](https://github.com/scrapy-plugins/scrapy-djangoitem)\n\n## SETUP:\n\n## 1. Install docker, compose\n\ninstall docker\n> wget -qO- https://get.docker.com/ | sh\nsudo usermod -a -G docker `whoami`\n\n install compose\n> sudo wget -q https://github.com/docker/compose/releases/download/1.6.2/docker-compose-`uname -s`-`uname -m` \\\n    -O /usr/local/bin/docker-compose\n> sudo chmod +x /usr/local/bin/docker-compose\n\n**Tip** : after that, logout, then login for update environment\n\n---------\n\n## 2. Run docker django-dynamic-scraper\n\n- clone git onfta-crawler\n> git clone git@gitlab.com:hv-db04/onfta-crawler.git\n> cd onfta-crawler/django_dynamic_scraper\n\n- run docker onfta-crawler\n> docker-compose up -d\n> docker exec -it <id container> bash\n\n---------\n\n## 3. Defining the object to be scraped\n\n\n\n\n- create Database utf8\n>  CREATE DATABASE news CHARACTER SET utf8 COLLATE utf8_general_ci;\n> $cd djangoItem\n\n- create user admin\n>  python manage.py createsuperuser\n\n- run django server\n>  python manage.py runserver 0.0.0.0:8000\n\n- show admin django in browser\n>  http://localhost:8000/admin\n\n> + add New Scraped object classes\n> + add New Scrapers\n> + add News websites\n\n---------\n\n## 4. run crawl data\n\n\n> **run**:\n\n**script:** ` scrapy crawl [--output=FILE --output-format=FORMAT] SPIDERNAME -a id=REF_OBJECT_ID [-a do_action=(yes|no) -a run_type=(TASK|SHELL) -a max_items_read={Int} -a max_items_save={Int} -a max_pages_read={Int} -a output_num_mp_response_bodies={Int} -a output_num_dp_response_bodies={Int} ] `\n\n>  scrapy crawl news -a id=1 -a do_action=yes\n\n-----------------------------\n## 5. run schedule crawl:\n\n>  **deploy project scrapy**:\n\n> - cd crawl\n> - scrapyd-deploy -p crawl\n> - scrapyd\n\n---------\n\n> **run schedule scrapy**:\n\n**script:** `python manage.py celeryd -l info -B --settings=example_project.settings`\n\n> python manage.py celeryd -l info -B --settings=djangoItem.settings\n\n---------\n>**run check  error expath**:\n\n**script:** `scrapy crawl news_checker -a id=ITEM_ID -a do_action=yes`\n",
  "dockerfile": "FROM ubuntu:14.04\nMAINTAINER khainguyen \"khainguyenptiter@gmail.com\"\n\nRUN apt-get update\n\nRUN apt-get install -y gcc g++ python-dev libssl-dev libxml2-dev libxslt1-dev libffi-dev libmysqlclient-dev python-pip python-mysqldb\nRUN apt-get install -y libtiff5-dev libjpeg8-dev zlib1g-dev libfreetype6-dev liblcms2-dev libwebp-dev tcl8.6-dev tk8.6-dev python-tk\nRUN pip install --upgrade six\nRUN pip install scrapyd-client\nCOPY requirements.txt /\n\nRUN pip install -r /requirements.txt\n\nEXPOSE 8000\nEXPOSE 6800\n",
  "dockerfile_json": {
    "add": [],
    "expose": [
      8000,
      6800
    ],
    "volume": [],
    "run": [
      "apt-get update",
      "apt-get install -y gcc g++ python-dev libssl-dev libxml2-dev libxslt1-dev libffi-dev libmysqlclient-dev python-pip python-mysqldb",
      "apt-get install -y libtiff5-dev libjpeg8-dev zlib1g-dev libfreetype6-dev liblcms2-dev libwebp-dev tcl8.6-dev tk8.6-dev python-tk",
      "pip install --upgrade six",
      "pip install scrapyd-client",
      "pip install -r /requirements.txt"
    ],
    "workdir": [],
    "from": "ubuntu:14.04",
    "maintainer": "khainguyen \"khainguyenptiter@gmail.com\"",
    "copy": "requirements.txt /"
  },
  "source_repository_url": "https://github.com/khainguyen95/django-dynamic-scraper.git",
  "source_repository_type": "git",
  "source_repository_provider": "github",
  "source_repository_web_url": "https://github.com/khainguyen95/django-dynamic-scraper",
  "docker_repository": "khainguyendinh/django-dynamic-scraper",
  "docker_image": "khainguyendinh/django-dynamic-scraper",
  "parameters_schema": {
    "exposed_ports": {
      "default": [
        8000,
        6800
      ],
      "type": "json_array"
    }
  },
  "type": "docker",
  "docker_name": "khainguyendinh/django-dynamic-scraper",
  "revision": "latest",
  "uris": [
    "https://hub.docker.com/r/khainguyendinh/django-dynamic-scraper",
    "https://hub.docker.com/v2/repositories/khainguyendinh/django-dynamic-scraper",
    "https://github.com/khainguyen95/django-dynamic-scraper.git"
  ],
  "labels": [
    "Docker",
    "Type/Middleware/Runtime/Python",
    "Mode/Executable/Image/VM Image/AMI"
  ],
  "info_url": "https://hub.docker.com/r/khainguyendinh/django-dynamic-scraper",
  "requires": [
    {
      "kind": "host",
      "label": "Docker Engine"
    }
  ],
  "latest": true,
  "gatherbase_origin": "docker-hub"
}