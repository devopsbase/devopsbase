{
  "dockerhub": {
    "web_url": "https://hub.docker.com/r/ianneub/elasticsearch-to-s3",
    "repository_url": "https://hub.docker.com/v2/repositories/ianneub/elasticsearch-to-s3",
    "tags_url": "https://hub.docker.com/v2/repositories/ianneub/elasticsearch-to-s3/tags",
    "dockerfile_url": "https://hub.docker.com/v2/repositories/ianneub/elasticsearch-to-s3/dockerfile",
    "autobuild_url": "https://hub.docker.com/v2/repositories/ianneub/elasticsearch-to-s3/autobuild",
    "user": "ianneub",
    "name": "elasticsearch-to-s3",
    "namespace": "ianneub",
    "status": 1,
    "is_private": false,
    "is_automated": true,
    "star_count": 0,
    "pull_count": 274,
    "last_updated": "2016-10-15T02:53:40.010270Z",
    "permissions": {
      "read": true,
      "write": false,
      "admin": false
    },
    "tags": [
      {
        "name": "latest",
        "full_size": 287961096,
        "id": 3560952,
        "repository": 762806,
        "creator": 14479,
        "last_updater": 14479,
        "last_updated": "2016-10-15T02:53:39.649157Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "import",
        "full_size": 287964212,
        "id": 3638465,
        "repository": 762806,
        "creator": 14479,
        "last_updater": 14479,
        "last_updated": "2016-10-15T02:53:33.337365Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "export",
        "full_size": 287965333,
        "id": 3638464,
        "repository": 762806,
        "creator": 14479,
        "last_updater": 14479,
        "last_updated": "2016-10-15T02:46:33.279357Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      }
    ],
    "build_name": "ianneub/elasticsearch-to-s3"
  },
  "name": "ianneub/elasticsearch-to-s3 Docker container",
  "description": "Export an Elasticsearch index to S3",
  "readme": "# Elasticsearch-to-S3\n\nEasily export an Elasticsearch index to Amazon S3. The image will export the index to a gzip file on Amazon S3.\n\n## Example\n\n```docker run --rm -e \"ES_URL=http://my.elasticsearch.server.com/\" -e \"ES_INDEX=my-index\" -e \"S3_BUCKET=my-bucket\" -e \"S3_KEY=es-test.gz\" ianneub/elasticsearch-to-s3```\n\n## Configuration\n\nAll configuration is done using environment variables. See the table below for all configuration settings.\n\n| Variable | Default | Description |\n| -------- | ------- | ----------- |\n| `ES_URL` | _none_ | This is the Elasticsearch server URL. |\n| `ES_INDEX` | _none_ | This is the Elasticsearch index you want to export. |\n| `S3_BUCKET` | _none_ | This is the S3 bucket that the exported data will be saved to. |\n| `S3_KEY` | _none_ | This is the name of the key that will be saved in the S3 bucket. |\n| `AWS_ACCESS_KEY_ID` | _none_ | Your AWS credentials. Do not specify if you want to use an IAM profile while opperating in EC2. |\n| `AWS_SECRET_ACCESS_KEY` | _none_ | Your AWS credentials. Do not specify if you want to use an IAM profile while opperating in EC2. |\n| `AWS_DEFAULT_REGION` | `us-east-1` | The region your S3 bucket is in. |\n\n## Output file specs\nThe file that will be output to S3 is in gzip format. The output file consists of multiple lines of JSON. The first line is a header JSON string that contains information about the index. Each following line is a JSON string representing a document from the Elasticsearch index.\n\n### Header\nThe first line in the output contains JSON with information about the index. It has the following keys:\n\n| Key | Description |\n| --- | ----------- |\n| settings | This is the settings for the index returned from the Elasticsearch [index settings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-settings.html) API. |\n| mapping | This is the mapping for the index returned from the Elasticsearch [mapping](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-mapping.html) API. |\n\n## Development\n\nDevelopment of this docker image uses Docker Compose. Simply install docker and then run the following commands to get going:\n\n1. `docker-compose up -d es`, then wait about 10 seconds for Elasticsearch to start up.\n2. `docker-compose up fill` to load some dummy data into Elasticsearch.\n3. `docker-compose up export` to run the export script.\n3. `docker-compose up import` to run the import script.\n",
  "dockerfile": "FROM ruby:2.3-onbuild\n\nENV AWS_DEFAULT_REGION=us-east-1\n\nCMD [\"ruby\", \"import.rb\"]\n",
  "dockerfile_json": {
    "add": [],
    "expose": [],
    "volume": [],
    "run": [],
    "workdir": [],
    "from": "ruby:2.3-onbuild",
    "env": {
      "AWS_DEFAULT_REGION": "us-east-1"
    },
    "cmd": "[\"ruby\", \"import.rb\"]"
  },
  "source_repository_url": "https://github.com/ianneub/elasticsearch-to-s3.git",
  "source_repository_type": "git",
  "source_repository_provider": "github",
  "source_repository_web_url": "https://github.com/ianneub/elasticsearch-to-s3",
  "docker_repository": "ianneub/elasticsearch-to-s3",
  "docker_image": "ianneub/elasticsearch-to-s3",
  "parameters_schema": {
    "AWS_DEFAULT_REGION": {
      "default": "us-east-1",
      "type": "string",
      "mapping": "environment_variable"
    },
    "exposed_ports": {
      "default": [],
      "type": "json_array"
    }
  },
  "type": "docker",
  "docker_name": "ianneub/elasticsearch-to-s3",
  "revision": "latest",
  "uris": [
    "https://hub.docker.com/r/ianneub/elasticsearch-to-s3",
    "https://hub.docker.com/v2/repositories/ianneub/elasticsearch-to-s3",
    "https://github.com/ianneub/elasticsearch-to-s3.git"
  ],
  "labels": [
    "Docker",
    "Binding/Region/Europe/EU",
    "Type/Infrastructure/Operating System",
    "Type/Middleware/Search/Elasticsearch"
  ],
  "info_url": "https://hub.docker.com/r/ianneub/elasticsearch-to-s3",
  "requires": [
    {
      "kind": "host",
      "label": "Docker Engine"
    }
  ],
  "latest": true,
  "gatherbase_origin": "docker-hub"
}