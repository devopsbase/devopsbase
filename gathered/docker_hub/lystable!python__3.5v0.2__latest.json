{
  "dockerhub": {
    "web_url": "https://hub.docker.com/r/lystable/python",
    "repository_url": "https://hub.docker.com/v2/repositories/lystable/python",
    "tags_url": "https://hub.docker.com/v2/repositories/lystable/python/tags",
    "dockerfile_url": "https://hub.docker.com/v2/repositories/lystable/python/dockerfile",
    "autobuild_url": "https://hub.docker.com/v2/repositories/lystable/python/autobuild",
    "user": "lystable",
    "name": "python",
    "namespace": "lystable",
    "status": 1,
    "is_private": false,
    "is_automated": true,
    "star_count": 0,
    "pull_count": 5628,
    "last_updated": "2016-09-26T16:59:30.656384Z",
    "permissions": {
      "read": true,
      "write": false,
      "admin": false
    },
    "tags": [
      {
        "name": "3.5v0.2",
        "full_size": 170438413,
        "id": 5071317,
        "repository": 398800,
        "creator": 383141,
        "last_updater": 383141,
        "last_updated": "2016-09-26T16:59:30.317083Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "3.5v0.1",
        "full_size": 170438413,
        "id": 5069755,
        "repository": 398800,
        "creator": 383141,
        "last_updater": 383141,
        "last_updated": "2016-09-26T16:33:30.940799Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "3.5",
        "full_size": 176816846,
        "id": 1826961,
        "repository": 398800,
        "creator": 383141,
        "last_updater": 383141,
        "last_updated": "2016-09-26T15:49:44.337079Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      },
      {
        "name": "develop",
        "full_size": 155959381,
        "id": 1476425,
        "repository": 398800,
        "creator": 90949,
        "last_updater": 298643,
        "last_updated": "2016-08-03T11:18:27.366723Z",
        "image_id": null,
        "v2": true,
        "platforms": [
          5
        ]
      }
    ],
    "build_name": "lystable/infrastructure"
  },
  "name": "lystable/python Docker container",
  "description": "Python 3.5 base image",
  "readme": "# Contents\n### [Create AWS EC2 Instances (deprecated, use Fabric instructions in lystable-salt)](aws/README.md)\n### [Lystable's Database Setup in AWS](aws-rds/README.md)\n### [Produce Production / Staging CSRs or Self-Signed certificates](ssl/docker/README.md)\n### [Service Registration to Internal DNS with Route53](service-registration/README.md)\n### [Service Discovery with Consul](service-discovery/README.md)\n### [Service Alerting with Consul Alerts](service-alerting/README.md)\n### [Nginx Proxy for *.lystable App to S3](nginx-app/README.md)\n### [How To Access the Production VPN](vpn/README.md)\n### [Inbound Comms - Email via Mandrill](inbound-email-api/README.md)\n### [Calendar Integration - Google](google-calendar/README.md)\n\n# AWS Administration Tools\n\n- https://console.aws.amazon.com\n- the administrator VMs (named admin-prod and admin-stage) can execute commands such as a PSQL shell. Read [Lystable's Database Setup in AWS](aws-rds/README.md) for the DB configurations.\n\n# Architecture\n\n![alt text](./LystablePlatformDiagram.png)\n\n## Accessible publically\n\n- [Production Graphs](http://graphs.production.aws.lystable.com)\n- [Staging Graphs](http://graphs.staging.aws.lystable.com)\n\n## Accessible inside the VPN\n\n- [Production Sentry](http://sentry.production.aws:9000)\n- [Production Monitoring ElasticSearch Head](http://elasticsearch.production.aws:9200/_plugin/head)\n- [Production LWS Search ElasticSearch Head](http://lws-es.production.aws:9200/_plugin/head)\n- [Production API Stats](http://kibana3.production.aws:9090)\n- [Production Consul UI](http://consul.production.aws:8500/ui)\n- [Staging Sentry](http://sentry.staging.aws:9000)\n- [Staging ElasticSearch Head](http://elasticsearch.staging.aws:9200/_plugin/head)\n- [Staging LWS Search ElasticSearch Head](http://lws-es.staging.aws:9200/_plugin/head)\n- [Staging Consul UI](http://consul.staging.aws:8500/ui)\n\n# SSH / VPN\n\n## SSH\n\nFollow the instructions on the [Salt project README](https://github.com/lystable/lystable-salt/blob/develop/README.md)\nto setup your SSH.\n\n## VPN\n\nFollow these [instructions](vpn/README.md).\n\n# Services\n\n- LWS\n- LWS-DB which represents an on-demand service to run migrations\n- Sentry\n- Elasticsearch / Logstash / Kibana4\n- Consul for service discovery\n- DNS service register, mini-service to register containers on Route53\n- Consul Alerts for alerting based on Consul healthchecks\n\n# Proxies\n\nWe have 2 Nginx proxy configurations, one which is front-facing and one\nwhich we use for admin purposes.\n\n## lystable.com\n\nServing\n\n- ```**www.lystable.com**``` marketing website over HTTP **only**\n- ```**\\*.lystable.com**``` team app\n- ```**(www.)?lystable.com/me**``` supplier app\n- ```**(www.)?lystable.com/<username>**``` future supplier login page\n\nThis proxy is run on multiple instances.\n\nThe Salt role name is **lystable-com-proxy** and it uses the **nginx** state.\n\n## *.[production|staging].aws.lystable.com\n\nServing\n\n**[graphs|deployment].[staging|production].aws.lystable.com**\n\n- The graphs are the Kibana4 UI\n- The deployment is the Salt Master HTTP API called by Circle\n\nThis proxy is run on multiple instances.\n\nThe Salt role name is **admin-proxy** and it uses the **nginx** state.\n\n# AWS\n\n## EC2\n\nWe're using environment and role \"tags\" to determine which service are\ndeployed on each EC2 VM.\n\nEnvironment names and role names can be found in the [Salt project](https://github.com/lystable/lystable-salt/blob/develop/pillars/top.sls)\n\nE.g. this section indicates that the Elasticsearch / Logstash / Kibana services\nshould be deployed on any VM which has the tags ```environment=production``` and ```role=es```\n\n```\n    'G@environment:production and G@role:es':\n      - match: compound\n      - production.elasticsearch\n      - production.logstash\n      - production.kibana\n      - production.kibana3\n```\n\n### Attach and Mount EBS Storage\n\nFollow these instructions http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html\n\n## RDS\n\n### Access Postgres Shell\n\n    ssh admin-prod # 'admin-stage' for staging\n\n    sudo su\n\n    psql-lws\n\n### Access LWS Shell\n\n    ssh limelight\n\n    sudo su\n\n    salt -C 'G@role:lws' ls\n\nTake your pick of a host name, e.g. ip-10-1-20-187.ec2.internal\n\n    ssh 10.1.20.187\n\n    sudo su\n\n    docker exec -ti lws bash\n\n    lws-cmd shell\n\n\n### Creating snapshots of production onto staging\n\n- Go to the RDS service in AWS and then to Snapshots\n- Select the most recent snapshot of 'lws-production'\n- Restore snapshot with:\n  - Multi-AZ = No\n  - VPC = staging\n  - Availability Zone = us-east-1b\n  - Identifier = lws-staging-<date of today>\n- Wait for the instance to publish its DNS name, it will look like \"lws-staging-21.cpcx9cs8xjiz.us-east-1.rds.amazonaws.com\"\n- Copy that into the entry \"lws.rds.staging.aws\" in Route 53\n- SSH into Salt master with `ssh dwelyn\n- Become sudo\n- Run\n```\nsalt '*' cmd.run 'docker rm -f lws nameko-userconnections nameko-mail nameko-reminders nameko-socialmedia nameko-googlecalendar nameko-adminapp lws-admin'\n```\n- Re-trigger the last LWS release circle build\n\n\n# Docker Host\n\nHost: `docker.lystable.ink`\n\nTo create:\n\n`docker-machine create --driver generic --generic-ssh-key ~/.ssh/aws/lws.pem --generic-ssh-user ubuntu --generic-ip-address=docker.lystable.ink remote`\n\n# Deployment\n\n## Trigger deployments\n\n### Circle\n\n- LWS:\n  - any ```release/*``` branch build will deploy onto staging\n  - any ```production``` branch build will deploy onto production\n\n- Frontend:\n  - any ```release/*``` branch build will just deploy to the staging S3 buckets\n  - any ```production``` branch build will archive the app build\ninto an S3 bucket ```lystable-app-releases``` with the release version number.\nAfter that it will deploy onto the production buckets.\n\n### Salt\n\nTriggering a production deployment via Salt can be done like:\n\n    ssh limelight # 'dwelyn' for staging\n\n    sudo su\n\n    # Deploy only LWS migrations\n    salt -C 'G@role:lws-db' state.apply lws-db pillar='{lws_db_version: <current LWS version>}'\n\n    # Deploy only LWS\n    salt -C 'G@role:lws' state.apply lws pillar='{lws_version: <current LWS version>}'\n\n    # Deploy only Nameko workers\n    salt -C 'G@role:nameko' state.apply nameko-worker pillar='{nameko_version: <current LWS version>}'\n\nFor the remainder role names and their state names, read the [top pillar file](https://github.com/lystable/lystable-salt/blob/develop/pillars/top.sls)\n\n### Rollback or Change Version of the Team / Supplier apps\n\nThe different release versions of the apps are archived during the build of\nthe release branches into an S3 bucket called ```lystable-app-releases```.\n\nIn order to rollback the apps to version v0.4.0:\n\n    aws s3 rm s3://team.lystable.com/ --recursive\n    aws s3 cp s3://lystable-app-releases/team/0.4.0/ s3://team.lystable.com/ --recursive\n    aws s3 rm s3://supplier.lystable.com/ --recursive\n    aws s3 cp s3://lystable-app-releases/supplier/0.4.0/ s3://supplier.lystable.com/ --recursive\n\n\n\n## Change container configuration\n\nAll configurations for production are under the lystable-salt/pillars/production/ and lystable-salt/pillars/staging/ directories in the lystable-salt project.\n\nTo change the LWS configuration, for example we would test it in staging first (see below) and then run it in production.\n\nThe best way to deploy it is by re-triggering the last release build, since it will trigger all the LWS-related deployments that need the configuration. Otherwise configuration can be tested just for LWS containers, as a sample by following the below steps.\n\n### Test in staging\n\n```\n    ssh dwelyn\n\n    sudo su\n\n    cd ~/lystable-salt/\n\n    vi pillars/staging/lws-env.sls # update an environment variable. ATTENTION: the dockerng salt state does not like integers, please string-quote them\n\n    # As per the deployment section previously\n    salt -C 'G@role:lws' state.apply lws pillar='{lws_version: <current LWS version>}'\n```\n\n### Merge it and update production\n\n- Push for a PR\n\n```\n    git checkout -b feature/config-change\n    ... commit and push ...\n```\n \n- Merge the PR into develop\n\n- Update prod and stage Salt masters\n\n```\n    ssh dwelyn\n    sudo su\n    cd ~/lystable-salt\n    git checkout develop\n    git pull\n\n    ssh limelight\n    sudo su\n    cd ~/lystable-salt\n    git checkout develop\n    git pull\n```\n\n### Deploy it in production\n\n- Re-trigger the last LWS release build to update the configuration of all LWS-related components\n\n**Note:** There remains one case where the containers need to be restarted\nafterwards, and that is when there are mounted configurations. This is true currently\nfor:\n\n- Logstash\n\nSo, to update Logstash configuration:\n\n    # ... carry out the steps listed above first ...\n\n    # and then\n\n    salt -C 'G@role:es' cmd.run 'docker restart logstash'\n\n# Monitoring\n\n## Python errors\n\n- [Production Sentry](http://sentry.production.aws:9000)\n- [Staging Sentry](http://sentry.staging.aws:9000)\n\nYou need to be on the corresponding VPN to access these.\n\n### How to Suppress Specific Sentry Hipchat Notifications\n\n1. Navigate to:\n    - [Staging Sentry Rules](http://sentry.staging.aws:9000/lystable/lws/rules/)\n    - [Production Sentry Rules](http://sentry.production.aws:9000/lystable/lws/rules/)\n2. Enter a rule set and specify or remove rules. **Current approach:** go all out and start adding rules specific to an error message when we know and stop caring about it.\n\nTake example of existing rules in there.\n\n## Running services\n\n- [Production Consul UI](http://consul.production.aws:8500/ui)\n- [Staging Consul UI](http://consul.staging.aws:8500/ui)\n\nYou need to be on the corresponding VPN to access these.\n\n## Service down alerts\n\nWe use Consul Alerts, readme [here](service-alerting/README.md).\n\n## Getting container logs\n\nAPI logs and application exceptions by LWS, Nameko, LWS DB are logged in files under /mnt/<service name> on the host machine.\n\nuWSGI exceptions like OOM are visible in the `docker logs lws` stdout.\n\nFor example to check a failed DB deployment:\n\n- note the hostname of the failed box printed in Slack, it will look like ip-10-0-20-106.ec2.internal\n- derive the IP from that and SSH into it with just \"ssh 10.0.20.106\"\n- check the latest logs in /mnt/lws-db/ on the VM in question.\n\n## Handling AWS Alerts\n\n- You get an alert with the name \"Disk-utilisation-prod-services-21\"\n- You log in to the EC2 console on AWS and find the instance named \"prod-services-21\"\n- Check if the state of it (visible in EC2 instance listing)\n- If the state is now \"OK\" and the issue was memory or CPU then it probably doesn't need diving into\n- If the state is now \"OK\" but the issue was disk then we should still dive into it\n- If the state is still \"ALARM\" then.. dive in ;)\n- You get its IP address\n- SSH into the box\n```\nssh <ip>\n```\n\n### Disk space\n\n- Check remaining disk space\n```\ndf -h\n```\n- Run this command on root, and then in nested directories until the problematic files are found\n```\ndu -hs /*\n# Then perhaps\ndu -hs /mnt/*\n# Or\ndu -hs /var/lib/docker/*\n```\n\n#### Common causes\n\n- Docker volumes haven't been cleaned up, so run:\n```\ndocker volume ls -qf dangling=true | xargs -r docker volume rm; rm -rf /var/lib/docker/tmp/*\n```\n- Logs need rotating (obviously this loses a few log lines in the process...)\n```\ntar -cvf lws.log.20160720.tar lws.log\ngzip lws.log.20160720.tar\nrm -f lws.log lws.log.20160720.tar\n```\n- Logs need deleting. `elasticsearch-query.log` for example is not valuable and needs to stop being generated altogether.\n\n### Memory \n\n- Watch `top` to find the problematic process\n- Check service logs\n\n#### Causes we had so far\n\n- Crazy DB queries which also locked a large number of resources. Resolution was:\n    - Log into psql prompt\n    - Use `pg_stat_activity` and `pg_locks` to find the problematic queries. Use `pid` and `relation_id` to find which query is blocked by which query and kill the one that's crazy\n- ES reindexing (at 5am UTC)\n- Prod data snapshot for dev usage (at 6am UTC)\n\n### CPU\n\n- Watch `top` to find the problematic process\n- Check service logs\n\n#### Causes we had so far\n\n- Crazy DB queries (same as the memory scenario described above)\n- ES reindexing (at 5am UTC)\n- Prod data snapshot for dev usage (at 6am UTC)\n\n# KPIs\n\nWe are extracting a number of tables from Postgres and into Elasticsearch using Logstash, primarily for tracking KPIs based on user activity on the platform.\n\n## Kibana4\n\nThe UIs with the graphs are accessible publically:\n\n- [Production Graphs](http://graphs.production.aws.lystable.com)\n- [Staging Graphs](http://graphs.staging.aws.lystable.com)\n\n## Logstash\n\nThe Postgres tables dumped into Elasticsearch are configured via a Salt configuration of [Logstash in YAML](https://github.com/lystable/lystable-salt/blob/develop/pillars/production/logstash.sls).\n\nFor example, to extract the reviews table:\n\n    review:\n      query: 'select reviews.id, reviews.created_at, users.email as reviewing_user, teams.display_name as reviewing_team, suppliers.name as reviewed_supplier, sentiment, value_for_money, quality from reviews inner join users on users.id = reviews.created_by inner join teams on teams.id = reviews.team_id inner join suppliers on suppliers.id = reviews.supplier_id'\n      type: review\n      schedule: '* * * * *'\n      time_column: reviews.created_at\n      only_last: '3 minutes'\n\n**Deployment Note:** See the Deployment section above for how to update.\n\n# Inbound Email\n\nWe use [Mandrill](https://mandrillapp.com) for inbound email from users.\n\n## Configuration\n\nWe have 2 webhooks currently configured for this, staging and production:\n\n- https://mandrill-inbound-email.lystable.com/comms/messages/content/\n- https://mandrill-inbound-email.lystable.net/comms/messages/content/\n\nEmails can be CC'ed or sent to inbound@lystable.com for production, and mandrill-inbound-email.lystable.net for staging.\n\n# Maintenance\n\nAs per the [Ubuntu upgrade instructions](https://wiki.ubuntu.com/Security/Upgrades), all outstanding security updates will be installed via the following command:\n\n```\nsudo apt-get update\nsudo DEBIAN_FRONTEND=noninteractive apt-get -y -o Dpkg::Options::=\"--force-confdef\" -o Dpkg::Options::=\"--force-confold\" dist-upgrade\n```\n",
  "dockerfile": "FROM library/python:3.5.0-slim\n\nRUN apt-get update\n\nRUN apt-get install -y --no-install-recommends gcc libpq-dev libxml2-dev make git && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN apt-get autoremove -y\n\n",
  "dockerfile_json": {
    "add": [],
    "expose": [],
    "volume": [],
    "run": [
      "apt-get update",
      "apt-get install -y --no-install-recommends gcc libpq-dev libxml2-dev make git &&     rm -rf /var/lib/apt/lists/*",
      "apt-get autoremove -y"
    ],
    "workdir": [],
    "from": "library/python:3.5.0-slim"
  },
  "source_repository_url": "git@github.com:lystable/infrastructure.git",
  "source_repository_type": "git",
  "source_repository_provider": "github",
  "source_repository_web_url": "https://github.com/lystable/infrastructure",
  "docker_repository": "lystable/python",
  "docker_image": "lystable/python",
  "parameters_schema": {
    "exposed_ports": {
      "default": [],
      "type": "json_array"
    }
  },
  "type": "docker",
  "docker_name": "lystable/python",
  "revision": "3.5v0.2",
  "uris": [
    "https://hub.docker.com/r/lystable/python",
    "https://hub.docker.com/v2/repositories/lystable/python",
    "git@github.com:lystable/infrastructure.git"
  ],
  "labels": [
    "Docker",
    "Type/Middleware/Runtime/Python",
    "Mode/Executable/Image"
  ],
  "info_url": "https://hub.docker.com/r/lystable/python",
  "requires": [
    {
      "kind": "host",
      "label": "Docker Engine"
    }
  ],
  "latest": true,
  "gatherbase_origin": "docker-hub"
}